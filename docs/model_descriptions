Models:
(more accurate and full model descriptions are avaliable in model folders on Minio)


no_pretraining_cosine_loss -- странная моделька, из берта еще даже не предобученного на наших данных, кроме того, в процессе обучения постепенно расходился MLM. Параметры обучения неизвестны. Шаги вдоль градиента MLM не использовались. Разве что очень странный бейзлайн.

naive_cosine.* -- семейство моделей обученных с чуть разным LR, оптимизация вдоль MLM градиента
	
	- high_lr -- по-видимому, лучше всего сошелся косинус, где-то около 0.1 (GOOD MODEL) 
	- SWA -- не работает, модель убилась при повышении LR (может быть интересно понять, насколько умерло)
	- EMA -- тоже неплохо сошелся косинус, если поучить с LR повыше, могло бы получиться даже лучше. (GOOD MODEL)
	- low_lr -- слишком низкий LR, если поставить повыше, могло бы и сойтись (тестить не интересно)

svd_projection.* -- семейство моделей с лоссом, посчитанным с дополнительной регуляризацией подпространства, содержащего полиперсональные вектора. По большей части, MLM ломается в процессе обучения.

	-- standard (стоило бы переназвать в WholeWordMasking, моделька плохо понимает, что происходит)
	-- -5_with_pretrained_bert_mlm, 3,-2_with_pretrained_bert_mlm_lower_lr -- сходится cos, расходится MLM
	-- -5_with_pretrained_bert_mlm_lower_lr -- постепенно сходится cos, MLM стабилен. Возможно, стоит продолжить обучение. Но может быть интересно и на текущем этапе потестить.

Plain_mlm.* -- тесты того, что анизотропия в берте не наша выдумка (просто базовые берты)

