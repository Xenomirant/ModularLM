{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkRjTgTvHt0C"
   },
   "source": [
    "# Bert Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jESjuwfQHw8c"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SSzfVu-SnBOl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers[torch] datasets evaluate wandb minio tqdm scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K8_hHVCNnBOq"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pickle as pkl\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BertForMaskedLM,\n",
    "    get_scheduler,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "# from maskedtensor import masked_tensor\n",
    "\n",
    "from tqdm.auto import trange, tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import io\n",
    "import pathlib\n",
    "from PairsDataset import PairsDataset\n",
    "\n",
    "import wandb\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import logging\n",
    "import ema_swa_utils\n",
    "import SVDmatrix\n",
    "from collections import deque\n",
    "\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn')\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRY_NAME = \"naive_cosine_with_pretrained_bert_high_lr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KTpeEhBwIBdl"
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 16\n",
    "MLM_PROB = 0.15\n",
    "\n",
    "#DATA_PATH = '/content/drive/MyDrive/nnlp/bert/biblioteka_prikluchenij_both_agr.csv'\n",
    "DATA_PATH = \"data/train_dataset.csv\"\n",
    "TEST_PATH = \"data/tda_test.csv\"\n",
    "MODEL_NAME = 'DeepPavlov/rubert-base-cased'\n",
    "WEIGHTS_PATH = \"ckpt/pretrained_bert_epoch_9.999976796259556.pt\"\n",
    "\n",
    "USE_SWA = False\n",
    "SWA_START = 4\n",
    "SWA_LR = 5e-2\n",
    "\n",
    "USE_EMA = False\n",
    "EMA_DECAY = 0.999\n",
    "\n",
    "# whether to log the layers being changed (happens once per notebook restart)\n",
    "LOG_LAYERS = True\n",
    "\n",
    "# full, batch, none\n",
    "# actually full and batch anisotropy are different -- the first is for bigger matrices of embeddings\n",
    "# of base and changed language -- and the batch-wise calculates it based on the matrix of differences\n",
    "# TODO: make it a relevant parameter, currently full is computed all the time\n",
    "COMPUTE_ANISOTROPY = \"full\"\n",
    "SAVE_ANISOTROPY_MATRIX = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zi_u2NiL0mgC",
    "outputId": "cbeb7da4-03ef-40c1-a015-90ec071ab357"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 7e-3\n",
    "\n",
    "n_mlm = 1\n",
    "n_cosine = 10\n",
    "division_layer = 3\n",
    "weight_mlm = 1.2\n",
    "MEAN_OVER_CHANGED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DESCRIPTION = \\\n",
    "f'''\n",
    "\n",
    "Description: test of model with slightly higher mlm weight, but quite large learning rate with SWA\n",
    "\n",
    "Model: based on rubert, additionally pretrained for 10 epochs;\n",
    "Checkpoint: {WEIGHTS_PATH};\n",
    "Context: {SEQ_LEN};\n",
    "Batch size: {BATCH_SIZE};\n",
    "\n",
    "Loss: classic cosine embedding with steps along the MLM gradient;\n",
    "Loss only over diff in tokenization: {MEAN_OVER_CHANGED};\n",
    "N_MLM: {n_mlm};\n",
    "N_Cosine: {n_cosine};\n",
    "Division_layer: {division_layer};\n",
    "weight_mlm: {weight_mlm};\n",
    "weight_cosine: polynomial decay, sum to 1;\n",
    "\n",
    "LR_SCHEDULER: MultiStep;\n",
    "Initial learning rate: {learning_rate};\n",
    "Steps: {num_epochs}, \n",
    "Decay: 0.8,\n",
    "Epochs: {num_epochs};\n",
    "\n",
    "Additional parameters and notes:\n",
    "EMA: {USE_EMA};\n",
    "EMA_DECAY: {EMA_DECAY};\n",
    "\n",
    "SWA: {USE_SWA};\n",
    "SWA_START: {SWA_START};\n",
    "SWA_LR: {SWA_LR};\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f\"./logs/{TRY_NAME}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path(directory)\n",
    "path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=path.joinpath(\"run.log\"), filemode=\"w\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minio handler to use remote data -- implements get and put methods with pickling option (view file)\n",
    "\n",
    "from MinioHandler import MinioHandler\n",
    "\n",
    "minio = MinioHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMZUfmdLEBnR",
    "outputId": "75c4dd46-dd6c-4f31-ad83-74a0a565b17c"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZ1tRDh5RvDP",
    "outputId": "93a64300-36f2-4450-dded-ede6b6454b44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxenomirant\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "_XaOQ4gtSauA",
    "outputId": "94169790-8c91-4460-aaf1-56ab846703ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxenomirant\u001b[0m (\u001b[33mgrammar-bert\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/app/ModularLM/wandb/run-20240412_000941-buppxqaq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grammar-bert/grammar-bert-model1/runs/buppxqaq' target=\"_blank\">apricot-frost-157</a></strong> to <a href='https://wandb.ai/grammar-bert/grammar-bert-model1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grammar-bert/grammar-bert-model1' target=\"_blank\">https://wandb.ai/grammar-bert/grammar-bert-model1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grammar-bert/grammar-bert-model1/runs/buppxqaq' target=\"_blank\">https://wandb.ai/grammar-bert/grammar-bert-model1/runs/buppxqaq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/grammar-bert/grammar-bert-model1/runs/buppxqaq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7b8e187ad090>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project='grammar-bert-model1',\n",
    "    entity='grammar-bert'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdbf_sj8Hy_9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TQrOptuKvk4A"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(DATA_PATH, index_col = 0)\n",
    "# df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df.was_changed].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GB-e4FABxy8y"
   },
   "outputs": [],
   "source": [
    "# tqdm.pandas()\n",
    "\n",
    "# idx_init = df.initial.progress_apply(lambda x: x.replace(' ', ''))\n",
    "# idx_pol = df.polypers.progress_apply(lambda x: x.replace(' ', ''))\n",
    "# idx = -(idx_init == idx_pol)\n",
    "# df['was_changed'] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yrQu60MLz9cT"
   },
   "outputs": [],
   "source": [
    "# df.to_csv(DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(DATA_PATH, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(df, test_size=TEST_SIZE, stratify = df[\"was_changed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(\"data/train_bpa.csv\")\n",
    "# test.to_csv(\"data/test_bpa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pick items from test for TDA and homology computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/test_bpa.csv\", index_col = 0)\n",
    "\n",
    "# df = df[df.was_changed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tda_data = df.sample(n = 250, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tda_data.to_csv(\"tda_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minio.put_object(tda_data, save_name=\"data/tda_test.pkl\", pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything to minio -- also possible to use default minio functions from Minio class\n",
    "\n",
    "# minio.minio.fput_object(file_path=\"data/test_dataset.csv\", bucket_name=\"public\",\n",
    "#                       object_name=\"ModularLM/data/test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDe7I4QR4gVY"
   },
   "source": [
    "## Dataset and collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "V06Z3lby7xme"
   },
   "outputs": [],
   "source": [
    "def collate_func(batch):\n",
    "    batch = [data_collator.torch_call(item) for item in zip(*batch)]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7JP-0xw72PH",
    "outputId": "2f1ca2ee-6378-4dae-cd1e-3eac9d6235a9"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer.pad_token = '[SEP]'\n",
    "tokenizer.eos_token = '[SEP]'\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=MLM_PROB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fh94UBHqxuSA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MLM Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Mb9N8GAh_9Te"
   },
   "outputs": [],
   "source": [
    "# dt = datasets.Dataset.from_csv(DATA_PATH)\n",
    "# dt = dt.remove_columns(['polypers', 'was_changed']).rename_column('initial', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "d84hj1c4IQwT"
   },
   "outputs": [],
   "source": [
    "# N_samples = 10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6ZHrIWiQA_rn"
   },
   "outputs": [],
   "source": [
    "# def tokenize_function(example):\n",
    "#     return tokenizer(example['text'], truncation=True)\n",
    "\n",
    "# tok_dt = dt.select(range(N_samples)).map(tokenize_function, batched=True)\n",
    "# tok_dt = tok_dt.train_test_split(test_size=100,\n",
    "#                          shuffle=True,\n",
    "#                          seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "AKDqk2mzBQz-"
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     report_to = 'wandb',\n",
    "#     output_dir='part1-model',\n",
    "#     learning_rate=1e-3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     num_train_epochs=1,\n",
    "#     # evaluation_strategy='steps',\n",
    "#     # eval_steps=20,\n",
    "#     logging_steps=20,\n",
    "#     logging_first_step=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "F3MYlLtm76uJ"
   },
   "outputs": [],
   "source": [
    "# model = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "# model.to(device)\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "LC0KZCwqA50q"
   },
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tok_dt['train'],\n",
    "#     # eval_dataset=tok_dt['test'],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "7vEzJTIREz0k"
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8nu5cYyIxDd"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "4RB10XU9bMbH"
   },
   "outputs": [],
   "source": [
    "def save_gradients(model, division_layer):\n",
    "    layers = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        # division layer passed == division layer + 1 as is inside train\n",
    "        if name.startswith(f'bert.encoder.layer.{division_layer}'):\n",
    "            break\n",
    "        if (param.requires_grad) and param.grad is not None:\n",
    "            layers[name] = param.grad.detach().clone()\n",
    "    if LOG_LAYERS:\n",
    "        logger.info(f\"Saved layers: {str(layers.keys())}\")\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "qHcpG4G8cclV"
   },
   "outputs": [],
   "source": [
    "def change_gradients(*, model, layers, \n",
    "                     division_layer,\n",
    "                     weight_mlm=0.5, \n",
    "                     weight_cos=1, ):\n",
    "\n",
    "    global LOG_LAYERS\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        # division layer passed == division layer + 1 as is inside train\n",
    "        if name.startswith(f'bert.encoder.layer.{division_layer}'):\n",
    "            break\n",
    "        if name in layers:\n",
    "            param.grad = weight_cos * param.grad + weight_mlm * layers[name]\n",
    "            if LOG_LAYERS:\n",
    "                logger.info(f\"Changed layer: {name}\")\n",
    "                logger.info(f\"gradients changed. {(weight_cos * param.grad).norm(), (weight_mlm * layers[name]).norm()}\\n\")\n",
    "    LOG_LAYERS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "H9vdmHBsLAXg"
   },
   "outputs": [],
   "source": [
    "class CosWeightDecay:\n",
    "    '''\n",
    "    Cosine Weight with decaying step sizes after each multiplication\n",
    "    '''\n",
    "    def __init__(self, init_state=1, step=0.5):\n",
    "        self.cur_state = init_state\n",
    "        self.step = step\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        res = self.cur_state * other\n",
    "        self.cur_state = self.cur_state * self.step\n",
    "        return res\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.cur_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "H9vdmHBsLAXg"
   },
   "outputs": [],
   "source": [
    "class CosWeightSum2One:\n",
    "    '''\n",
    "    Cosine Weight summing to 1 over 10 steps (must be subject to change in case other step size is required)\n",
    "    This looks quite dumb...\n",
    "    '''\n",
    "    def __init__(self, init_coef = 100, steps: int = 10, linear = False):\n",
    "        self.counter = -1\n",
    "        if linear:\n",
    "            self.steps = init_coef*np.ones(steps)\n",
    "            return None\n",
    "        self.steps = init_coef*np.arange(2, steps+2)**-1.5\n",
    "        return None\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        res = self.steps[self.counter] * other\n",
    "        return res\n",
    "    \n",
    "    def step(self):\n",
    "        self.counter+=1\n",
    "        return None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.steps[self.counter])\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = -1\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.steps[self.counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "RsNP-PPvQayL"
   },
   "outputs": [],
   "source": [
    "class CosLoss:\n",
    "    def __init__(self, vector=None, alpha=0):\n",
    "        self.loss = nn.CosineEmbeddingLoss()\n",
    "        self.target = torch.ones(BATCH_SIZE).to(model.device)\n",
    "        self.alpha = alpha\n",
    "        self.vector = vector\n",
    "\n",
    "    def __call__(self, hid_ref, hid_cur, target):\n",
    "        cos_loss = self.loss(hid_ref, hid_cur, target)\n",
    "        if self.vector is not None:\n",
    "            cos_loss += self.alpha * self.loss(self.vector, hid_ref - hid_cur,\n",
    "                                               self.target)\n",
    "        return cos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "74aJSuBCtRcI"
   },
   "outputs": [],
   "source": [
    "def train(model, criteria, optimizer, lr_scheduler, data, hom_data, n_epochs=1,\n",
    "          n_cosine=10, division_layer=4, weight_mlm=1,\n",
    "          weight_cos=1, save_every_epoch=3, test_every=5000):\n",
    "\n",
    "    # global mlm_losses, cosine_losses\n",
    "    # change global loss tracking to local only -- for now it seems unnecessary\n",
    "    global tda_save_dict\n",
    "\n",
    "    executor = ThreadPoolExecutor(max_workers=3)\n",
    "\n",
    "    tq_epoch = trange(n_epochs, desc='Epochs: ')\n",
    "    tq_batch = tqdm(total=len(data))\n",
    "\n",
    "    # target for cosine loss\n",
    "    target = -torch.ones(BATCH_SIZE).to(model.device)\n",
    "    grads = None\n",
    "    \n",
    "    # just initialization -- first few batches make no difference for tracking\n",
    "    cos_loss = 0\n",
    "    mlm_loss = 0\n",
    "    hom_computed = 0\n",
    "    anisotropy_matrix = dict()\n",
    "    anisotropy_matrix[\"base\"] = None\n",
    "    anisotropy_matrix[\"polypers\"] = None\n",
    "    # TODO -- optimize for gradual saving of dict intead of accumulation\n",
    "\n",
    "    def anisotropy_computation(name, matrix, other):\n",
    "\n",
    "        if matrix is None:\n",
    "            matrix = other\n",
    "        else:\n",
    "            matrix = torch.cat((matrix, other), dim = 0)\n",
    "        if matrix.shape[0] == 4096:\n",
    "            proc = mp.Process(target=SVDmatrix.compute_anisotropy, \n",
    "                              kwargs={\"matrix\":matrix,\n",
    "                                      \"write_to_file\": path.joinpath(f\"{name}.anisotropy\")},\n",
    "                             daemon=True)\n",
    "            proc.start()\n",
    "            return None\n",
    "        return matrix\n",
    "\n",
    "    # function to read global anisotropy computation results\n",
    "    def get_last_line(filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                lastline = float(deque(f, 1)[0])\n",
    "        except FileNotFoundError:\n",
    "            lastline = 0.0\n",
    "        return lastline\n",
    "            \n",
    "\n",
    "    # save necessary features to dict\n",
    "    def save_tda_features(hom_data: torch.utils.data.DataLoader, save_dict: dict):\n",
    "\n",
    "        nonlocal hom_computed\n",
    "        hom_computed+=1\n",
    "        base = []\n",
    "        polypers = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i, batch in enumerate(hom_data):\n",
    "                # base embeddings after layer\n",
    "                pred_base = model(**{k: v.to(model.device) for k, v in batch[0].items()},\n",
    "                                  output_hidden_states=True)\n",
    "                hid_ref = torch.mean(pred_base.hidden_states[division_layer], dim=1)\n",
    "                base.extend(hid_ref.detach().cpu().numpy())\n",
    "\n",
    "                # polypers embeddings after layer\n",
    "                pred_new = model(**{k: v.to(model.device) for k, v in batch[1].items()},\n",
    "                                  output_hidden_states=True)\n",
    "                hid_cur = torch.mean(pred_new.hidden_states[division_layer], dim=1)\n",
    "                polypers.extend(hid_cur.detach().cpu().numpy())\n",
    "        \n",
    "        save_dict[hom_computed][\"base\"] = base\n",
    "        save_dict[hom_computed][\"polypers\"] = polypers\n",
    "        return save_dict\n",
    "\n",
    "    def gradient_norm():\n",
    "        grads = [\n",
    "        param.grad.detach().flatten()\n",
    "        for param in model.parameters()\n",
    "        if param.grad is not None\n",
    "        ]\n",
    "        norm = torch.cat(grads).norm()\n",
    "        return norm\n",
    "        \n",
    "    #########################################################\n",
    "    # training loop \n",
    "    #########################################################\n",
    "    for epoch in tq_epoch:\n",
    "        tq_batch.reset()\n",
    "        cosine_losses = [cos_loss]\n",
    "        mlm_losses = [mlm_loss]\n",
    "\n",
    "        for i, batch in enumerate(data):\n",
    "            # save data for TDA\n",
    "            if i % test_every == 0:\n",
    "                tda_save_dict = save_tda_features(hom_data, tda_save_dict)\n",
    "                # also a point of optimization\n",
    "                minio.put_object(tda_save_dict, \n",
    "                                 save_name=f\"data/TDA_FEATURES/tda_save_dict_{TRY_NAME}.pkl\", \n",
    "                                 pickle=True)\n",
    "                model.train()\n",
    "            # pred on base text    \n",
    "            pred = model(**{k: v.to(model.device) for k, v in batch[0].items()},\n",
    "                         output_hidden_states=True, )\n",
    "\n",
    "            ######################################\n",
    "            # MLM loss update ####################\n",
    "            ######################################\n",
    "            # once upon 10 steps, compute mlm loss\n",
    "            if i % n_cosine == 0:\n",
    "                pred.loss.backward(retain_graph=True)\n",
    "                grads = save_gradients(\n",
    "                    model=model, \n",
    "                    division_layer=division_layer\n",
    "                )\n",
    "                mlm_grad_norm = gradient_norm()\n",
    "\n",
    "                if (USE_EMA | USE_SWA):\n",
    "                    avg_model.update_parameters(model)\n",
    "                    # if we use SWA we need to redefine \n",
    "                    if USE_SWA: \n",
    "                        if epoch >= SWA_START:\n",
    "                            # it's supposed we won't use regular scheduler after SWA start\n",
    "                            lr_scheduler = swa_scheduler\n",
    "\n",
    "                # WARNING! we take lr step only every cycle, not batch\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                mlm_losses.append(pred.loss.detach().cpu())\n",
    "                # reset cosine weight every cycle\n",
    "                weight_cos.reset()\n",
    "\n",
    "            ###########################################\n",
    "            ###########################################\n",
    "            \n",
    "            # compute cosine anyway\n",
    "            # pred on polypers text\n",
    "            pred_new = model(**{k: v.to(model.device) for k, v in batch[1].items()},\n",
    "                             output_hidden_states=True)\n",
    "            \n",
    "            hid_ref = pred.hidden_states[division_layer]\n",
    "            hid_cur = pred_new.hidden_states[division_layer]\n",
    "            \n",
    "            # look for changed ids only\n",
    "            if MEAN_OVER_CHANGED:\n",
    "                mask = (batch[0][\"input_ids\"] - batch[1][\"input_ids\"]) != 0\n",
    "                mask = mask.unsqueeze(-1).expand(-1, -1, 768).to(model.device)\n",
    "                # masked tensors don't support loss calculations as filling with 0 stops differentiation\n",
    "                # hid_ref = masked_tensor(hid_ref, mask.to(model.device), requires_grad=True).to_tensor(value=0)\n",
    "                # hid_cur = masked_tensor(hid_cur, mask.to(model.device), requires_grad=True).to_tensor(value=0)\n",
    "                hid_ref = hid_ref*mask\n",
    "                hid_cur = hid_cur*mask\n",
    "\n",
    "            \n",
    "            hid_ref = torch.mean(hid_ref, dim=1)\n",
    "            hid_cur = torch.mean(hid_cur, dim=1)\n",
    "\n",
    "            # compute batch anisotropy\n",
    "            thread = executor.submit(SVDmatrix.compute_anisotropy, matrix=(hid_cur-hid_ref).detach().cpu())\n",
    "\n",
    "            # process classes one by one\n",
    "            anisotropy_matrix[\"base\"] = anisotropy_computation(name=\"base\", \n",
    "                                                               matrix=anisotropy_matrix[\"base\"],\n",
    "                                                              other=hid_ref.detach().cpu())\n",
    "            anisotropy_matrix[\"polypers\"] = anisotropy_computation(name=\"polypers\", \n",
    "                                                               matrix=anisotropy_matrix[\"polypers\"],\n",
    "                                                              other=hid_cur.detach().cpu())\n",
    "                \n",
    "            \n",
    "            cos_loss = criteria(hid_ref, hid_cur, target)\n",
    "            cos_loss.backward()\n",
    "            cos_grad_norm = gradient_norm()\n",
    "            \n",
    "            # as indexing in weight_cos starts from -1, take a step before applying            \n",
    "            weight_cos.step()\n",
    "\n",
    "            # new_grads = {}\n",
    "            # for name, parameter in model.named_parameters():\n",
    "            #     if name in grads:\n",
    "            #         new_grads[name] = parameter.grad.clone()\n",
    "\n",
    "            change_gradients(model=model, \n",
    "                             layers=grads, \n",
    "                             division_layer=division_layer,\n",
    "                             weight_mlm=weight_mlm, \n",
    "                             weight_cos=weight_cos)\n",
    "            \n",
    "            # with open(\"grad_log\", \"a\") as f:\n",
    "            #     for name, parameter in model.named_parameters():\n",
    "            #         if name in new_grads:\n",
    "            #             f.writelines(f\"{name} - {(new_grads[name] - parameter.grad).norm()}\")\n",
    "                \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()                    \n",
    "\n",
    "            cosine_losses.append(cos_loss.detach().cpu())\n",
    "\n",
    "            # some averaged estimate ro report\n",
    "            cos_loss = (sum(cosine_losses[-30:]) / len(cosine_losses[-30:])).item()\n",
    "            mlm_loss = (sum(mlm_losses[-30:]) / len(mlm_losses[-30:])).item()\n",
    "\n",
    "            # TODO: possibly send not every batch -- however, it acts asynchrously, so doesn't seem to make much difference\n",
    "            wandb.log({\"Epoch\": epoch,\n",
    "                       \"MLM loss\": mlm_loss,\n",
    "                       \"Cosine loss\": cos_loss,\n",
    "                       \"MLM grad norm\": mlm_grad_norm,\n",
    "                       \"Cos grad norm\": cos_grad_norm,\n",
    "                       \"Cos Weight\": weight_cos.weight, # may be not that necessary to report unless we chaned it somehow\n",
    "                       \"Learning rate\": lr_scheduler.get_last_lr()[0],\n",
    "                       \"Local anisotropy\": thread.result(),\n",
    "                       \"Global anisotropy base\": get_last_line(path.joinpath(\"base.anisotropy\")),\n",
    "                       \"Global anisotropy polypers\": get_last_line(path.joinpath(\"polypers.anisotropy\")),\n",
    "                      })\n",
    "            tq_batch.set_postfix({\n",
    "                    'MLM loss': mlm_loss,\n",
    "                    'Cosine loss': cos_loss\n",
    "                })\n",
    "\n",
    "            tq_batch.update(1)\n",
    "\n",
    "        if epoch % save_every_epoch == 0:\n",
    "            # Note -- we don't save the model class, only the weights\n",
    "            print(\"Saving model checkpoint...\")\n",
    "            buffer = io.BytesIO()\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'mlm_loss': pred.loss,\n",
    "                    'cos_loss': cos_loss\n",
    "                            }, f=buffer)\n",
    "            # TODO -- add custom hash to model instead of value\n",
    "            minio.put_object(buffer.getvalue(), \n",
    "                             save_name=f\"ckpt/{TRY_NAME}/model_epoch_{epoch}.pt\")\n",
    "    \n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "SPNBA0emMpWr"
   },
   "outputs": [],
   "source": [
    "dt = PairsDataset(tokenizer, path=DATA_PATH)\n",
    "dl = DataLoader(dt, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                collate_fn=collate_func, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "SPNBA0emMpWr"
   },
   "outputs": [],
   "source": [
    "dt_tda = PairsDataset(tokenizer, path=TEST_PATH)\n",
    "dl_tda = DataLoader(dt_tda, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                collate_fn=collate_func, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "3I3B8JUjnBOs"
   },
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights from the last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = minio.get_object(WEIGHTS_PATH, type=\"model\")\n",
    "model_dict = torch.load(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_dict[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Compute SVD on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from SVDmatrix import GetSVD\n",
    "\n",
    "# get_svd = GetSVD(model=model, dataloader=dl, division_layer=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_svd.get_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(get_svd.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_svd.compute_svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minio.put_object(get_svd.matrix, save_name=\"3rd_layer_diff_matrix.pkl\", pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = minio.get_object(\"3rd_layer_diff_matrix.pkl\", unpickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minio.put_object(get_svd.svd, save_name=\"3rd_layer_diff_svd.pkl\", pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different solution (when library internal tobytes interface is implemented --\n",
    "# this variant is prefered)\n",
    "\n",
    "# minio.put_object(get_svd.svd[0].tobytes(), save_name=\"svd_test\", pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "CAq3W2Qd3c3y"
   },
   "outputs": [],
   "source": [
    "# scale by number of steps in a cycle\n",
    "weight_mlm /= n_cosine\n",
    "\n",
    "weight_cos = CosWeightSum2One(init_coef=1)\n",
    "#weight_cos = CosWeightSum2One(init_coef=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "vncR8UNMvk4C"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = name.startswith(f\"bert.encoder.layer.{division_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "mSRwfobbRsQY"
   },
   "outputs": [],
   "source": [
    "# vec = torch.normal(0.5,\n",
    "#                    0.1,\n",
    "#                    size=(768, ),\n",
    "#                    requires_grad=False).repeat(BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "O48t9N1K8XCF"
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=learning_rate)\n",
    "# criterion = CosLoss(alpha=0.5, vec=vec)\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "num_training_steps = int(num_epochs * len(dl) / n_cosine * n_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33640"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of lr sheduler updates\n",
    "num_training_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate & averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (USE_SWA):\n",
    "    \n",
    "    # lr_scheduler = get_scheduler(\n",
    "    #                 name=\"cosine\", optimizer=optimizer, num_warmup_steps=0,\n",
    "    #                 num_training_steps=num_training_steps\n",
    "    #                 )\n",
    "\n",
    "    # make steps at the end of first 5 epochs -- may be object to change made explicitely\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                        milestones=[int(len(dl)*i/n_cosine*n_mlm) for i in range(1, num_epochs)],\n",
    "                                                        gamma=0.8,)\n",
    "    \n",
    "    # # Note: Averaged model is applicable to custom modules -- not only full models, \n",
    "    # so it can be used for module training as well\n",
    "    if USE_EMA:\n",
    "        avg_model = ema_swa_utils.AveragedModel(model,  \n",
    "                                                multi_avg_fn=ema_swa_utils.get_ema_multi_avg_fn(decay=EMA_DECAY)).to(device)\n",
    "\n",
    "elif USE_SWA:\n",
    "    avg_model = ema_swa_utils.AveragedModel(model,\n",
    "                                           multi_avg_fn=ema_swa_utils.get_swa_multi_avg_fn()).to(device)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                        milestones=[int(len(dl)*i/n_cosine*n_mlm)for i in range(1, num_epochs)][:3],\n",
    "                                                        gamma=0.5,)\n",
    "    # ex. after 5 epochs\n",
    "    logger.info(f\"Using swa after {SWA_START} epochs\")\n",
    "    # grows swa learning rate from current optimizer value till the SWA_lr,\n",
    "    # anneal epochs determines the number of steps necessary\n",
    "    # may be it's plausible to change from our current implementation to the epoch-wise stepping\n",
    "    swa_scheduler = ema_swa_utils.SWALR(optimizer=optimizer, \n",
    "                                    swa_lr=SWA_LR,\n",
    "                                    anneal_strategy=\"cos\",\n",
    "                                    anneal_epochs=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Description: test of model with slightly higher mlm weight, but quite large learning rate with SWA\n",
      "\n",
      "Model: based on rubert, additionally pretrained for 10 epochs;\n",
      "Checkpoint: ckpt/pretrained_bert_epoch_9.999976796259556.pt;\n",
      "Context: 64;\n",
      "Batch size: 16;\n",
      "\n",
      "Loss: classic cosine embedding with steps along the MLM gradient;\n",
      "Loss only over diff in tokenization: True;\n",
      "N_MLM: 1;\n",
      "N_Cosine: 10;\n",
      "Division_layer: 3;\n",
      "weight_mlm: 1.2;\n",
      "weight_cosine: polynomial decay, sum to 1;\n",
      "\n",
      "LR_SCHEDULER: MultiStep;\n",
      "Initial learning rate: 0.001;\n",
      "Steps: 5, \n",
      "Decay: 0.5,\n",
      "Epochs: 10;\n",
      "\n",
      "Additional parameters and notes:\n",
      "EMA: False;\n",
      "EMA_DECAY: 0.999;\n",
      "\n",
      "SWA: False;\n",
      "SWA_START: 4;\n",
      "SWA_LR: 0.05;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396,
     "referenced_widgets": [
      "001cfa81c7354e93be3df3a5cf1b1058",
      "1957ad156733493da7a1d98af4e41f75",
      "b09cac7f786d41508fc087bfdc5967b7",
      "3adea4ff86b3452ea61fa306820297e6",
      "644be7a3004544af89d53d40dfad2f2b",
      "ab1566b360054b28942a3037cf3b746a",
      "36d21aa4eb7245f7918062cea3ea7a1b",
      "99f7527310794081b4b6adf4b6f3d4eb",
      "02c75806a08d4fd48c35b5a1dcdea1fb",
      "4064efa851f842cdb64b85b7498e2072",
      "9cd5f89775324214b3bcf755a575bf56",
      "7145096b67ab4dc69573e9047570c587",
      "e465332bfe814a2485071916e79d6ec0",
      "f468a1aab83f440a9a49912cb2cf901e",
      "cf17fde9c27f40a4b4e8ee7a83918f96",
      "e60d7f63aedf4b52982c846515cbc703",
      "991f151a7fe041e39275a97cdc1d2838",
      "edf64befa1a3437f82ca7886bb27b9af",
      "4c82641162ed4e569484ba69cbc255c6",
      "e1441e83a1f84e448f7a4b19dc1e6527",
      "99f562bfbd5c4f25a847a01add5d776a",
      "f76d3bb75a774ce9a538561618ca09cf"
     ]
    },
    "id": "F762mn4J1t5F",
    "outputId": "51fddd23-30ec-446f-fcde-ae5f7db8f13e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModularLM/ckpt/trained_models/naive_cosine_with_pretrained_bert_high_lr/DESCRIPTION.txt: |####################| 0.00 MB/0.00 MB 100% [elapsed: 00:00 left: 00:00,  0.21 MB/sec]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0e4221561c4ff69e67b9641f2b4ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19693d9b5f249af90473ef9c7633e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModularLM/data/TDA_FEATURES/tda_save_dict_naive_cosine_with_pretrained_bert_high_lr.pkl: |####################| 1.42 MB/1.42 MB 100% [elapsed: 00:00 left: 00:00, 4897.45 MB/sec]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModularLM/data/TDA_FEATURES/tda_save_dict_naive_cosine_with_pretrained_bert_high_lr.pkl: |####################| 9.95 MB/9.95 MB 100% [elapsed: 00:00 left: 00:00, 100.82 MB/sec]]Saving model checkpoint...\n",
      "ModularLM/data/TDA_FEATURES/tda_save_dict_naive_cosine_with_pretrained_bert_high_lr.pkl: |####################| 34.13 MB/34.13 MB 100% [elapsed: 00:00 left: 00:00, 126.17 MB/sec] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tda_save_dict \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhom_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_tda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m      \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m      \u001b[49m\u001b[43mn_cosine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_cosine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdivision_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdivision_layer\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m      \u001b[49m\u001b[43mweight_mlm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_cos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_cos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_every_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# in case we use averaging -- we need to recalculate batch | layer normalization before inference\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (USE_EMA \u001b[38;5;241m|\u001b[39m USE_SWA):\n",
      "Cell \u001b[0;32mIn[44], line 143\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criteria, optimizer, lr_scheduler, data, hom_data, n_epochs, n_cosine, division_layer, weight_mlm, weight_cos, save_every_epoch, test_every)\u001b[0m\n\u001b[1;32m    136\u001b[0m     weight_cos\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m###########################################\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m###########################################\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# compute cosine anyway\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# pred on polypers text\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m pred_new \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                 \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m hid_ref \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mhidden_states[division_layer]\n\u001b[1;32m    147\u001b[0m hid_cur \u001b[38;5;241m=\u001b[39m pred_new\u001b[38;5;241m.\u001b[39mhidden_states[division_layer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:1360\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1360\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1375\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py:206\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_tensors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has to be a tuple/list of tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m num_args_in_forward_chunk_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_args_in_forward_chunk_fn \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_tensors):\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_chunk_fn expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_args_in_forward_chunk_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m arguments, but only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensors are given\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3254\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_wrapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3255\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3002\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3000\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3001\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigcls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3003\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfollow_wrapper_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3004\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:2401\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2396\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is not a callable object\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj))\n\u001b[1;32m   2398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mMethodType):\n\u001b[1;32m   2399\u001b[0m     \u001b[38;5;66;03m# In this case we skip the first parameter of the underlying\u001b[39;00m\n\u001b[1;32m   2400\u001b[0m     \u001b[38;5;66;03m# function (usually `self` or `cls`).\u001b[39;00m\n\u001b[0;32m-> 2401\u001b[0m     sig \u001b[38;5;241m=\u001b[39m \u001b[43m_get_signature_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__func__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_bound_arg:\n\u001b[1;32m   2404\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _signature_bound_method(sig)\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:2463\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2458\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mreplace(parameters\u001b[38;5;241m=\u001b[39mnew_params)\n\u001b[1;32m   2460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2461\u001b[0m     \u001b[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2462\u001b[0m     \u001b[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[0;32m-> 2463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2464\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mskip_bound_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_bound_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2465\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[1;32m   2468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2469\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:2325\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2323\u001b[0m kind \u001b[38;5;241m=\u001b[39m _POSITIONAL_ONLY \u001b[38;5;28;01mif\u001b[39;00m posonly_left \u001b[38;5;28;01melse\u001b[39;00m _POSITIONAL_OR_KEYWORD\n\u001b[1;32m   2324\u001b[0m annotation \u001b[38;5;241m=\u001b[39m annotations\u001b[38;5;241m.\u001b[39mget(name, _empty)\n\u001b[0;32m-> 2325\u001b[0m parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[43mParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m posonly_left:\n\u001b[1;32m   2328\u001b[0m     posonly_left \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:2639\u001b[0m, in \u001b[0;36mParameter.__init__\u001b[0;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[1;32m   2637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, kind, \u001b[38;5;241m*\u001b[39m, default\u001b[38;5;241m=\u001b[39m_empty, annotation\u001b[38;5;241m=\u001b[39m_empty):\n\u001b[1;32m   2638\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kind \u001b[38;5;241m=\u001b[39m \u001b[43m_ParameterKind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   2641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid Parameter.kind\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/enum.py:385\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# simple value lookup\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_(\n\u001b[1;32m    388\u001b[0m         value,\n\u001b[1;32m    389\u001b[0m         names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         start\u001b[38;5;241m=\u001b[39mstart,\n\u001b[1;32m    394\u001b[0m         )\n",
      "File \u001b[0;32m/usr/lib/python3.10/enum.py:678\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEnum\u001b[39;00m(metaclass\u001b[38;5;241m=\u001b[39mEnumMeta):\n\u001b[1;32m    673\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    Generic enumeration.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03m    Derive from this class to define new enumerations.\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value):\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;66;03m# all enum instances are actually created during class construction\u001b[39;00m\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;66;03m# without calling this method; this method is called by the metaclass'\u001b[39;00m\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;66;03m# __call__ (i.e. Color(3) ), and by pickle\u001b[39;00m\n\u001b[1;32m    682\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(value) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;66;03m# For lookups like Color(Color.RED)\u001b[39;00m\n\u001b[1;32m    684\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# put description to minio before proceeding\n",
    "minio.put_object(DESCRIPTION, \n",
    "                save_name=f\"ckpt/trained_models/{TRY_NAME}/DESCRIPTION.txt\", pickle=True)\n",
    "\n",
    "# train model\n",
    "tda_save_dict = defaultdict(dict)\n",
    "\n",
    "try:\n",
    "    train(model=model, criteria=criterion, optimizer=optimizer,\n",
    "      lr_scheduler=lr_scheduler, data=dl, hom_data=dl_tda,\n",
    "      n_epochs=num_epochs,\n",
    "      n_cosine=n_cosine, division_layer=division_layer+1,\n",
    "      weight_mlm=weight_mlm, weight_cos=weight_cos,\n",
    "      save_every_epoch=3, test_every=5000)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # in case we use averaging -- we need to recalculate batch | layer normalization before inference\n",
    "    if (USE_EMA | USE_SWA):\n",
    "        torch.optim.swa_utils.update_bn(dl, avg_model)\n",
    "    \n",
    "    # save to trained models\n",
    "    logger.info(\"Saving model...\")\n",
    "    buffer = io.BytesIO()\n",
    "    torch.save({\n",
    "            'model_state_dict': avg_model.state_dict(),\n",
    "                    }, f=buffer)\n",
    "    \n",
    "    minio.put_object(buffer.getvalue(), \n",
    "                     save_name=f\"ckpt/trained_models/{TRY_NAME}/model.pt\")\n",
    "    \n",
    "    minio.fput_object(file_path=path.joinpath(\"base.anisotropy\"),\n",
    "                      object_name=f\"ckpt/trained_models/{TRY_NAME}/base.anisotropy.log\")\n",
    "    \n",
    "    minio.fput_object(file_path=path.joinpath(\"polypers.anisotropy\"),\n",
    "                      object_name=f\"ckpt/trained_models/{TRY_NAME}/polypers.anisotropy.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001cfa81c7354e93be3df3a5cf1b1058": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1957ad156733493da7a1d98af4e41f75",
       "IPY_MODEL_b09cac7f786d41508fc087bfdc5967b7",
       "IPY_MODEL_3adea4ff86b3452ea61fa306820297e6"
      ],
      "layout": "IPY_MODEL_644be7a3004544af89d53d40dfad2f2b"
     }
    },
    "02c75806a08d4fd48c35b5a1dcdea1fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1957ad156733493da7a1d98af4e41f75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab1566b360054b28942a3037cf3b746a",
      "placeholder": "​",
      "style": "IPY_MODEL_36d21aa4eb7245f7918062cea3ea7a1b",
      "value": "Epochs:   0%"
     }
    },
    "36d21aa4eb7245f7918062cea3ea7a1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3adea4ff86b3452ea61fa306820297e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4064efa851f842cdb64b85b7498e2072",
      "placeholder": "​",
      "style": "IPY_MODEL_9cd5f89775324214b3bcf755a575bf56",
      "value": " 0/1 [00:56&lt;?, ?it/s]"
     }
    },
    "4064efa851f842cdb64b85b7498e2072": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c82641162ed4e569484ba69cbc255c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "644be7a3004544af89d53d40dfad2f2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7145096b67ab4dc69573e9047570c587": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e465332bfe814a2485071916e79d6ec0",
       "IPY_MODEL_f468a1aab83f440a9a49912cb2cf901e",
       "IPY_MODEL_cf17fde9c27f40a4b4e8ee7a83918f96"
      ],
      "layout": "IPY_MODEL_e60d7f63aedf4b52982c846515cbc703"
     }
    },
    "991f151a7fe041e39275a97cdc1d2838": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99f562bfbd5c4f25a847a01add5d776a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99f7527310794081b4b6adf4b6f3d4eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cd5f89775324214b3bcf755a575bf56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab1566b360054b28942a3037cf3b746a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b09cac7f786d41508fc087bfdc5967b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99f7527310794081b4b6adf4b6f3d4eb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_02c75806a08d4fd48c35b5a1dcdea1fb",
      "value": 0
     }
    },
    "cf17fde9c27f40a4b4e8ee7a83918f96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99f562bfbd5c4f25a847a01add5d776a",
      "placeholder": "​",
      "style": "IPY_MODEL_f76d3bb75a774ce9a538561618ca09cf",
      "value": " 166/37379 [00:55&lt;2:56:01,  3.52it/s, Cosine loss=1.46]"
     }
    },
    "e1441e83a1f84e448f7a4b19dc1e6527": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e465332bfe814a2485071916e79d6ec0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_991f151a7fe041e39275a97cdc1d2838",
      "placeholder": "​",
      "style": "IPY_MODEL_edf64befa1a3437f82ca7886bb27b9af",
      "value": "  0%"
     }
    },
    "e60d7f63aedf4b52982c846515cbc703": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edf64befa1a3437f82ca7886bb27b9af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f468a1aab83f440a9a49912cb2cf901e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c82641162ed4e569484ba69cbc255c6",
      "max": 37379,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1441e83a1f84e448f7a4b19dc1e6527",
      "value": 166
     }
    },
    "f76d3bb75a774ce9a538561618ca09cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
