{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkRjTgTvHt0C"
   },
   "source": [
    "# Bert Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jESjuwfQHw8c"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SSzfVu-SnBOl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers[torch] datasets evaluate wandb minio tqdm scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gudhi ripser giotto-tda scikit-tda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K8_hHVCNnBOq"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForWholeWordMask,\n",
    "    DefaultDataCollator,\n",
    "    default_data_collator,\n",
    "    BertForMaskedLM,\n",
    "    get_scheduler,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "import training_utils\n",
    "from training_utils import *\n",
    "\n",
    "import transformers\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "# from maskedtensor import masked_tensor\n",
    "\n",
    "from tqdm.auto import trange, tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import io\n",
    "import pathlib\n",
    "from PairsDataset import PairsDataset\n",
    "\n",
    "import wandb\n",
    "\n",
    "# from IPython.display import clear_output\n",
    "import logging\n",
    "import ema_swa_utils\n",
    "import tda_utils\n",
    "from tda_utils import Stats\n",
    "import SVDmatrix\n",
    "from collections import deque\n",
    "\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn')\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRY_NAME = \"naive_cosine_with_pretrained_bert_EMA_updated_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KTpeEhBwIBdl"
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 16\n",
    "MLM_PROB = 0.15\n",
    "\n",
    "#DATA_PATH = '/content/drive/MyDrive/nnlp/bert/biblioteka_prikluchenij_both_agr.csv'\n",
    "DATA_PATH = \"data/train_dataset_with_masks.csv\"\n",
    "TEST_PATH = \"data/tda_test_with_masks.csv\"\n",
    "MODEL_NAME = 'DeepPavlov/rubert-base-cased'\n",
    "WEIGHTS_PATH = \"ckpt/pretrained_bert/model_epoch_10.pt\"\n",
    "\n",
    "USE_SWA = False\n",
    "SWA_START = 4\n",
    "SWA_LR = 5e-2\n",
    "\n",
    "USE_EMA = True\n",
    "EMA_DECAY = 0.995\n",
    "\n",
    "# whether to log the layers being changed (happens once per notebook restart)\n",
    "LOG_LAYERS = True\n",
    "\n",
    "# full, batch, none\n",
    "# actually full and batch anisotropy are different -- the first is for bigger matrices of embeddings\n",
    "# of base and changed language -- and the batch-wise calculates it based on the matrix of differences\n",
    "# TODO: make it a relevant parameter, currently full is computed all the time\n",
    "COMPUTE_ANISOTROPY = \"full\"\n",
    "SAVE_ANISOTROPY_MATRIX = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_utils.LOG_LAYERS = LOG_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zi_u2NiL0mgC",
    "outputId": "cbeb7da4-03ef-40c1-a015-90ec071ab357"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 13\n",
    "learning_rate = 5e-3\n",
    "num_warmup_steps = 3000\n",
    "\n",
    "n_mlm = 1\n",
    "n_cosine = 10\n",
    "division_layer = 3\n",
    "weight_mlm = 1.2\n",
    "weight_loss = 1\n",
    "USE_MASKS = True\n",
    "USE_TDA_MASKS = True\n",
    "\n",
    "# we save model checkpoint before weight update in case of huge mlm loss jumps, this is the threshold, when to save\n",
    "mlm_jump_threshold = 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DESCRIPTION = \\\n",
    "f'''\n",
    "{TRY_NAME.upper()}\n",
    "\n",
    "Description: Based on BERT pretrained for 10 epochs -- cosine loss with no additions, trained using EMA;\n",
    "\n",
    "Model: based on rubert, additionally pretrained for 10 epochs;\n",
    "Checkpoint: {WEIGHTS_PATH};\n",
    "Context: {SEQ_LEN};\n",
    "Batch size: {BATCH_SIZE};\n",
    "\n",
    "Loss: Cosine embedding loss; \n",
    "MLM type: token masking;\n",
    "\n",
    "Loss only over diff in tokenization: {USE_MASKS};\n",
    "Compute test tda on masked tokens only: {USE_TDA_MASKS};\n",
    "N_MLM: {n_mlm};\n",
    "N_Cosine: {n_cosine};\n",
    "Division_layer: {division_layer};\n",
    "weight_mlm: {weight_mlm};\n",
    "weight_loss: {weight_loss}, polynomial decay, sum to one;\n",
    "\n",
    "\n",
    "LR_SCHEDULER: Cosine annealing with warmup;\n",
    "Initial learning rate: {learning_rate};\n",
    "Warmup_steps: {num_warmup_steps};\n",
    "Steps: {num_epochs}, \n",
    "Decay: cosine,\n",
    "Epochs: {num_epochs};\n",
    "\n",
    "Additional parameters and notes:\n",
    "EMA: {USE_EMA};\n",
    "EMA_DECAY: {EMA_DECAY};\n",
    "\n",
    "SWA: {USE_SWA};\n",
    "SWA_START: {SWA_START};\n",
    "SWA_LR: {SWA_LR};\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f\"./logs/{TRY_NAME}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path(directory)\n",
    "path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=path.joinpath(\"run.log\"), filemode=\"w\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minio handler to use remote data -- implements get and put methods with pickling option (view file)\n",
    "\n",
    "from MinioHandler import MinioHandler\n",
    "\n",
    "minio = MinioHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMZUfmdLEBnR",
    "outputId": "75c4dd46-dd6c-4f31-ad83-74a0a565b17c"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZ1tRDh5RvDP",
    "outputId": "93a64300-36f2-4450-dded-ede6b6454b44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxenomirant\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "_XaOQ4gtSauA",
    "outputId": "94169790-8c91-4460-aaf1-56ab846703ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxenomirant\u001b[0m (\u001b[33mgrammar-bert\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/app/ModularLM/wandb/run-20240504_180800-3gwbyuxm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grammar-bert/grammar-bert-model1/runs/3gwbyuxm' target=\"_blank\">naive_cosine_with_pretrained_bert_EMA_updated_masks</a></strong> to <a href='https://wandb.ai/grammar-bert/grammar-bert-model1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grammar-bert/grammar-bert-model1' target=\"_blank\">https://wandb.ai/grammar-bert/grammar-bert-model1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grammar-bert/grammar-bert-model1/runs/3gwbyuxm' target=\"_blank\">https://wandb.ai/grammar-bert/grammar-bert-model1/runs/3gwbyuxm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/grammar-bert/grammar-bert-model1/runs/3gwbyuxm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7581a036f5b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project='grammar-bert-model1',\n",
    "    entity='grammar-bert', \n",
    "    name=TRY_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdbf_sj8Hy_9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TQrOptuKvk4A"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(DATA_PATH, index_col = 0)\n",
    "# df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df.was_changed].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GB-e4FABxy8y"
   },
   "outputs": [],
   "source": [
    "# tqdm.pandas()\n",
    "\n",
    "# idx_init = df.initial.progress_apply(lambda x: x.replace(' ', ''))\n",
    "# idx_pol = df.polypers.progress_apply(lambda x: x.replace(' ', ''))\n",
    "# idx = -(idx_init == idx_pol)\n",
    "# df['was_changed'] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yrQu60MLz9cT"
   },
   "outputs": [],
   "source": [
    "# df.to_csv(DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slightly more complicated masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_mask(base: dict, other: dict):\n",
    "#     '''\n",
    "#     Creates mask based on tokenizer output \n",
    "#     (really dumb solution but it somehow works)\n",
    "#     '''\n",
    "#     base_mask = np.zeros_like(base[\"input_ids\"])\n",
    "#     other_mask = np.zeros_like(other[\"input_ids\"])\n",
    "\n",
    "#     base_ids = base[\"input_ids\"]\n",
    "#     other_ids = other[\"input_ids\"]\n",
    "    \n",
    "#     j = 0\n",
    "#     i = 0\n",
    "#     base_counter = 1\n",
    "#     other_counter = 1\n",
    "#     flag = False\n",
    "    \n",
    "#     while True:\n",
    "#         # try:\n",
    "#         #     print(i, j, other_ids[i], base_ids[j], \n",
    "#         #           tokenizer.convert_ids_to_tokens(other_ids[i]), tokenizer.convert_ids_to_tokens(base_ids[j]), other_counter, base_counter)\n",
    "#         # except IndexError:\n",
    "#         #     pass\n",
    "#         try:\n",
    "#             if base_ids[j] == other_ids[i]:\n",
    "#                 if tokenizer.convert_ids_to_tokens(other_ids[i]).startswith(\"##\"):\n",
    "#                     other_counter += 1\n",
    "#                 else:\n",
    "#                     other_counter = 1\n",
    "        \n",
    "#                 if tokenizer.convert_ids_to_tokens(base_ids[j]).startswith(\"##\"):\n",
    "#                     base_counter += 1\n",
    "#                 else:\n",
    "#                     base_counter = 1\n",
    "                \n",
    "#                 i += 1\n",
    "#                 j += 1\n",
    "#                 continue\n",
    "                \n",
    "#             flag = True\n",
    "#             flag2 = True\n",
    "#             while flag:\n",
    "        \n",
    "#                 if tokenizer.convert_ids_to_tokens(other_ids[i]).startswith(\"##\"):\n",
    "#                     i += 1\n",
    "#                     other_counter += 1\n",
    "#                     flag2 = False\n",
    "    \n",
    "#                 if tokenizer.convert_ids_to_tokens(base_ids[j]).startswith(\"##\"):\n",
    "#                     j += 1\n",
    "#                     base_counter += 1\n",
    "#                     flag2 = False\n",
    "\n",
    "#                 if not any([tokenizer.convert_ids_to_tokens(other_ids[i]).startswith(\"##\"),\n",
    "#                            tokenizer.convert_ids_to_tokens(base_ids[j]).startswith(\"##\")]):\n",
    "#                     flag = False\n",
    "#                     if flag2:\n",
    "#                         i+=1\n",
    "#                         j+=1\n",
    "                        \n",
    "            \n",
    "#             base_mask[j-base_counter:j] = [1]*base_counter\n",
    "#             other_mask[i-other_counter:i] = [1]*other_counter\n",
    "#             base_counter = 1\n",
    "#             other_counter = 1\n",
    "                    \n",
    "#         except IndexError:\n",
    "#             break\n",
    "\n",
    "#     base = np.zeros(SEQ_LEN, dtype=int)\n",
    "#     other = np.zeros(SEQ_LEN, dtype=int)\n",
    "\n",
    "#     base[:len(base_mask)] = base_mask[:SEQ_LEN]\n",
    "#     other[:len(other_mask)] = other_mask[:SEQ_LEN]\n",
    "    \n",
    "#     return base, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base = []\n",
    "# polypers = []\n",
    "\n",
    "# for i in tqdm(range(len(dt.dataset))):\n",
    "\n",
    "#     base_mask, polypers_mask = create_mask(tokenizer(dt.dataset[\"base\"][i]), tokenizer(dt.dataset[\"polypers\"][i]))\n",
    "\n",
    "#     base.append(base_mask)\n",
    "#     polypers.append(polypers_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(DATA_PATH, index_col=0)\n",
    "# dataset = data[(data.was_changed)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"base_mask\"] = base\n",
    "# dataset[\"polypers_mask\"] = polypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_csv(\"data/train_dataset_with_masks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minio.fput_object(object_name=\"ModularLM/data/train_dataset_with_masks.csv\", file_path=\"./data/train_dataset_with_masks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_mask(base: dict, other: dict):\n",
    "#     '''\n",
    "#     Creates mask based on tokenizer output \n",
    "#     (really dumb solution but it somehow works)\n",
    "#     '''\n",
    "#     base_mask = np.zeros_like(base[\"input_ids\"])\n",
    "#     other_mask = np.zeros_like(other[\"input_ids\"])\n",
    "\n",
    "#     base_ids = base[\"input_ids\"]\n",
    "#     other_ids = other[\"input_ids\"]\n",
    "    \n",
    "#     cur = 0\n",
    "#     i = 0\n",
    "#     counter = 0\n",
    "#     token_counter = 1\n",
    "#     flag = False\n",
    "    \n",
    "#     while True:\n",
    "#         try:\n",
    "#             print(i, cur, other_ids[i], base_ids[cur], \n",
    "#                   tokenizer.convert_ids_to_tokens(other_ids[i]), tokenizer.convert_ids_to_tokens(base_ids[cur]),\n",
    "#                  end = \" \")\n",
    "#         except IndexError:\n",
    "#             pass\n",
    "#         try:\n",
    "#             if other_ids[i] == base_ids[cur]:\n",
    "#                 if flag:\n",
    "#                     base_mask[cur-token_counter:cur] = [1]*token_counter\n",
    "#                     flag = False\n",
    "#                 if tokenizer.convert_ids_to_tokens(base_ids[cur]).startswith(\"##\"):    \n",
    "#                     token_counter += 1\n",
    "#                 else:\n",
    "#                     token_counter = 1\n",
    "#                 cur += 1\n",
    "#                 i += 1\n",
    "#                 print(token_counter)\n",
    "#                 continue\n",
    "#         except IndexError:\n",
    "#             break\n",
    "#         print()\n",
    "#         if tokenizer.convert_ids_to_tokens(other_ids[i]).startswith(\"##\"):\n",
    "#             other_mask[i-token_counter:i+1] = [1]*(token_counter+1)\n",
    "#             flag = True\n",
    "#             counter += 1\n",
    "#             i += 1\n",
    "#             continue\n",
    "#         else: \n",
    "#             # base_mask[curtoken_counter-1:cur] = [1]*(token_counter)\n",
    "#             # flag = False\n",
    "#             cur += token_counter\n",
    "#             continue\n",
    "#         cur += 1\n",
    "#         i += 1\n",
    "   \n",
    "#     return base_mask, other_mask        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(DATA_PATH, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(df, test_size=TEST_SIZE, stratify = df[\"was_changed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv(\"data/train_bpa.csv\")\n",
    "# test.to_csv(\"data/test_bpa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pick items from test for TDA and homology computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/test_bpa.csv\", index_col = 0)\n",
    "\n",
    "# df = df[df.was_changed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tda_data = df.sample(n = 250, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tda_data.to_csv(\"tda_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minio.put_object(tda_data, save_name=\"data/tda_test.pkl\", pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything to minio -- also possible to use default minio functions from Minio class\n",
    "\n",
    "# minio.minio.fput_object(file_path=\"data/test_dataset.csv\", bucket_name=\"public\",\n",
    "#                       object_name=\"ModularLM/data/test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDe7I4QR4gVY"
   },
   "source": [
    "## Dataset and collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tda_collate_func(batch):\n",
    "    if USE_MASKS:\n",
    "        ref = [item[0] for item in batch]\n",
    "        cur = [item[1] for item in batch]\n",
    "        mask_ref = torch.BoolTensor([(item[2]) for item in batch])\n",
    "        mask_cur = torch.BoolTensor([(item[3]) for item in batch])\n",
    "        return [def_data_collator(ref), def_data_collator(cur), mask_ref, mask_cur]\n",
    "    batch = [torch.tensor(item) for item in zip(*batch)]\n",
    "    return batch\n",
    "\n",
    "\n",
    "def collate_func(batch):\n",
    "    if USE_MASKS:\n",
    "        ref = [item[0] for item in batch]\n",
    "        cur = [item[1] for item in batch]\n",
    "        mask_ref = torch.BoolTensor([item[2] for item in batch])\n",
    "        mask_cur = torch.BoolTensor([item[3] for item in batch])\n",
    "        return [data_collator.torch_call(ref), data_collator.torch_call(cur), mask_ref, mask_cur]\n",
    "    batch = [data_collator.torch_call(item) for item in zip(*batch)]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer.pad_token = '[SEP]'\n",
    "tokenizer.eos_token = '[SEP]'\n",
    "## note the change -- we now mask the whole words independent of tokenization\n",
    "# data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm_probability=MLM_PROB)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=MLM_PROB)\n",
    "def_data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3I3B8JUjnBOs"
   },
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights from the last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = minio.get_object(WEIGHTS_PATH, type=\"model\")\n",
    "model_dict = torch.load(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_dict[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fh94UBHqxuSA"
   },
   "source": [
    "## MLM Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Mb9N8GAh_9Te"
   },
   "outputs": [],
   "source": [
    "# dt = datasets.Dataset.from_csv(DATA_PATH)\n",
    "# dt = dt.remove_columns(['polypers', 'was_changed']).rename_column('initial', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "d84hj1c4IQwT"
   },
   "outputs": [],
   "source": [
    "# N_samples = 10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "6ZHrIWiQA_rn"
   },
   "outputs": [],
   "source": [
    "# def tokenize_function(example):\n",
    "#     return tokenizer(example['text'], truncation=True)\n",
    "\n",
    "# tok_dt = dt.select(range(N_samples)).map(tokenize_function, batched=True)\n",
    "# tok_dt = tok_dt.train_test_split(test_size=100,\n",
    "#                          shuffle=True,\n",
    "#                          seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "AKDqk2mzBQz-"
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     report_to = 'wandb',\n",
    "#     output_dir='part1-model',\n",
    "#     learning_rate=1e-3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     num_train_epochs=1,\n",
    "#     # evaluation_strategy='steps',\n",
    "#     # eval_steps=20,\n",
    "#     logging_steps=20,\n",
    "#     logging_first_step=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "F3MYlLtm76uJ"
   },
   "outputs": [],
   "source": [
    "# model = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "# model.to(device)\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "LC0KZCwqA50q"
   },
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tok_dt['train'],\n",
    "#     # eval_dataset=tok_dt['test'],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "7vEzJTIREz0k"
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criteria, optimizer, lr_scheduler, data, hom_data, n_epochs=1,\n",
    "          n_cosine=10, division_layer=4, weight_mlm=1,\n",
    "          weight_loss=1, save_every_epoch=3, test_every=5000):\n",
    "\n",
    "    # global mlm_losses, cosine_losses\n",
    "    # change global loss tracking to local only -- for now it seems unnecessary\n",
    "    global tda_save_dict\n",
    "\n",
    "    executor = ThreadPoolExecutor(max_workers=6)\n",
    "\n",
    "    tq_epoch = trange(n_epochs, desc='Epochs: ')\n",
    "    tq_batch = tqdm(total=len(data))\n",
    "\n",
    "    # target for cosine loss\n",
    "    target = -torch.ones(BATCH_SIZE).to(model.device)\n",
    "    grads = None\n",
    "    \n",
    "    # just initialization -- first few batches make no difference for tracking\n",
    "    total_loss = 0\n",
    "    mlm_loss = 0\n",
    "    hom_computed = 0\n",
    "    mlm_jumps = 0\n",
    "    anisotropy_matrix = dict()\n",
    "    anisotropy_matrix[\"base\"] = None\n",
    "    anisotropy_matrix[\"polypers\"] = None\n",
    "    # TODO -- optimize for gradual saving of dict intead of accumulation\n",
    "\n",
    "    def global_computation(name, matrix, other):\n",
    "\n",
    "        if matrix is None:\n",
    "            matrix = other\n",
    "        else:\n",
    "            matrix = torch.cat((matrix, other), dim = 0)\n",
    "        if matrix.shape[0] == 4096:\n",
    "            proc = mp.Process(target=SVDmatrix.compute_anisotropy, \n",
    "                              kwargs={\"matrix\":matrix,\n",
    "                                      \"write_to_file\": path.joinpath(f\"{name}.anisotropy\")},\n",
    "                             daemon=True)\n",
    "            proc2 = mp.Process(target=tda_utils.calculate_ph_dim,\n",
    "                               kwargs={\"W\": matrix,\n",
    "                                      \"write_to_file\": path.joinpath(f\"{name}.ph_dim\")},\n",
    "                               daemon=True)\n",
    "\n",
    "            proc3 = mp.Process(target=tda_utils.compute_tda_features,\n",
    "                              kwargs={\"points\": matrix,\n",
    "                                     \"max_dim\": 1, \n",
    "                                     \"write_to_file\": path.joinpath(f\"{name}.tda_feats\")},\n",
    "                              daemon=True)\n",
    "            proc.start()\n",
    "            proc2.start()\n",
    "            proc3.start()\n",
    "            return None\n",
    "        return matrix\n",
    "\n",
    "    # function to read global anisotropy computation results\n",
    "    def get_last_line(filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                lastline = deque(f, 1)[0]\n",
    "        except FileNotFoundError:\n",
    "            lastline = \"0.0\"\n",
    "        return lastline\n",
    "\n",
    "    def read_tda_feats(filename):\n",
    "        feats = get_last_line(filename)\n",
    "        if feats == \"0.0\":\n",
    "            return Stats(entropy=.0, mean=.0, std=.0)\n",
    "            \n",
    "        return eval(feats)\n",
    "\n",
    "    # save necessary features to dict\n",
    "    def save_tda_features(hom_data: torch.utils.data.DataLoader, save_dict: dict):\n",
    "\n",
    "        nonlocal hom_computed\n",
    "        hom_computed+=1\n",
    "        base = []\n",
    "        polypers = []\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(hom_data):\n",
    "                # base embeddings after layer\n",
    "                pred_base = model(**{k: v.to(model.device) for k, v in batch[0].items()},\n",
    "                                  output_hidden_states=True)\n",
    "                hid_ref = pred_base.hidden_states[division_layer]\n",
    "                if USE_TDA_MASKS:\n",
    "                    hid_ref *= batch[2].unsqueeze(-1).to(model.device)\n",
    "                hid_ref = torch.mean(hid_ref, dim = 1)\n",
    "                base.extend(hid_ref.detach().cpu().numpy())\n",
    "\n",
    "                # polypers embeddings after layer\n",
    "                pred_new = model(**{k: v.to(model.device) for k, v in batch[1].items()},\n",
    "                                  output_hidden_states=True)\n",
    "                hid_cur = pred_new.hidden_states[division_layer]\n",
    "                if USE_TDA_MASKS:\n",
    "                    hid_cur *= batch[3].unsqueeze(-1).to(model.device)\n",
    "                hid_cur = torch.mean(hid_cur, dim = 1)\n",
    "                polypers.extend(hid_cur.detach().cpu().numpy())\n",
    "        \n",
    "        save_dict[hom_computed][\"base\"] = base\n",
    "        save_dict[hom_computed][\"polypers\"] = polypers\n",
    "        return save_dict\n",
    "\n",
    "    def save_model(epoch, model, optimizer, pred, total_loss, custom_name=None):\n",
    "        print(\"Saving model checkpoint...\")\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'mlm_loss': pred.loss,\n",
    "                'cos_loss': total_loss\n",
    "                        }, f=buffer)\n",
    "        # TODO -- add custom hash to model instead of value\n",
    "        if custom_name is None:\n",
    "            minio.put_object(buffer.getvalue(), \n",
    "                         save_name=f\"ckpt/{TRY_NAME}/model_epoch_{epoch}.pt\")\n",
    "        else:\n",
    "            minio.put_object(buffer.getvalue(), \n",
    "                         save_name=f\"ckpt/{TRY_NAME}/model_{custom_name}.pt\")\n",
    "        return None\n",
    "            \n",
    "\n",
    "    def gradient_norm():\n",
    "        grads = [\n",
    "        param.grad.detach().flatten()\n",
    "        for param in model.parameters()\n",
    "        if param.grad is not None\n",
    "        ]\n",
    "        norm = torch.cat(grads).norm()\n",
    "        return norm\n",
    "        \n",
    "    #########################################################\n",
    "    # training loop \n",
    "    #########################################################\n",
    "    for epoch in tq_epoch:\n",
    "        tq_batch.reset()\n",
    "        total_losses = [total_loss]\n",
    "        mlm_losses = [mlm_loss]\n",
    "\n",
    "        for i, batch in enumerate(data):\n",
    "            # save data for TDA\n",
    "            if i % test_every == 0:\n",
    "                tda_save_dict = save_tda_features(hom_data, tda_save_dict)\n",
    "                # also a point of optimization\n",
    "                minio.put_object(tda_save_dict, \n",
    "                                 save_name=f\"data/TDA_FEATURES/tda_save_dict_{TRY_NAME}.pkl\", \n",
    "                                 pickle=True)\n",
    "                test_base_pers_dim = executor.submit(tda_utils.calculate_ph_dim, W=np.array(\n",
    "                    tda_save_dict[hom_computed][\"base\"])\n",
    "                                                    )\n",
    "                test_polypers_pers_dim = executor.submit(tda_utils.calculate_ph_dim, W=np.array(\n",
    "                    tda_save_dict[hom_computed][\"polypers\"])\n",
    "                                                        )\n",
    "                test_base_tda_feats = executor.submit(tda_utils.compute_tda_features, points=np.array(\n",
    "                    tda_save_dict[hom_computed][\"base\"])\n",
    "                                                     )\n",
    "                test_polypers_tda_feats = executor.submit(tda_utils.compute_tda_features, points=np.array(\n",
    "                    tda_save_dict[hom_computed][\"polypers\"])\n",
    "                                                         )\n",
    "                model.train()\n",
    "            # pred on base text    \n",
    "            pred = model(**{k: v.to(model.device) for k, v in batch[0].items()},\n",
    "                         output_hidden_states=True, )\n",
    "\n",
    "            ######################################\n",
    "            # MLM loss update ####################\n",
    "            ######################################\n",
    "            # once upon 10 steps, compute mlm loss\n",
    "            if i % n_cosine == 0:\n",
    "                pred.loss.backward(retain_graph=True)\n",
    "                grads = save_gradients(\n",
    "                    model=model, \n",
    "                    division_layer=division_layer,\n",
    "                    logger=logger\n",
    "                )\n",
    "                mlm_grad_norm = gradient_norm()\n",
    "\n",
    "                if (USE_EMA | USE_SWA):\n",
    "                    avg_model.update_parameters(model)\n",
    "                    # if we use SWA we need to redefine \n",
    "                    if USE_SWA: \n",
    "                        if epoch >= SWA_START:\n",
    "                            # it's supposed we won't use regular scheduler after SWA start\n",
    "                            lr_scheduler = swa_scheduler\n",
    "\n",
    "                # WARNING! we take lr step only every cycle, not batch\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                mlm_losses.append(pred.loss.detach().cpu())\n",
    "                mlm_loss = (sum(mlm_losses[-30:]) / len(mlm_losses[-30:])).item()\n",
    "                if (mlm_losses[-1] >= mlm_loss*mlm_jump_threshold) and len(mlm_losses) > 100:\n",
    "                    save_model(epoch, model, optimizer, pred, total_loss, custom_name=f\"emergency_mlm_jump_{mlm_jumps}\")\n",
    "                    mlm_jumps += 1\n",
    "                    \n",
    "\n",
    "                \n",
    "                # reset cosine weight every cycle\n",
    "                weight_loss.reset()\n",
    "\n",
    "            ###########################################\n",
    "            ###########################################\n",
    "            \n",
    "            # compute cosine anyway\n",
    "            # pred on polypers text\n",
    "            pred_new = model(**{k: v.to(model.device) for k, v in batch[1].items()},\n",
    "                             output_hidden_states=True)\n",
    "            \n",
    "            hid_ref = pred.hidden_states[division_layer]\n",
    "            hid_cur = pred_new.hidden_states[division_layer]\n",
    "\n",
    "            # process classes one by one (if we need masked | avergaved vectors, we can just exchange the order in these few blocks)\n",
    "            anisotropy_matrix[\"base\"] = global_computation(name=\"base\", \n",
    "                                                               matrix=anisotropy_matrix[\"base\"],\n",
    "                                                              other=torch.mean(hid_ref.detach().clone().cpu(), dim=1))\n",
    "            anisotropy_matrix[\"polypers\"] = global_computation(name=\"polypers\", \n",
    "                                                               matrix=anisotropy_matrix[\"polypers\"],\n",
    "                                                              other=torch.mean(hid_cur.detach().clone().cpu(), dim=1))\n",
    "            \n",
    "            # look for changed ids only\n",
    "            if USE_MASKS:\n",
    "                # commented as deprecated for now\n",
    "                # 1. use diff in indices as masking (too simple)\n",
    "                # mask = (batch[0][\"input_ids\"] - batch[1][\"input_ids\"]) != 0\n",
    "                # mask = mask.unsqueeze(-1).expand(-1, -1, 768).to(model.device)\n",
    "                # 2. Not working but interesting\n",
    "                # # masked tensors don't support loss calculations as filling with 0 stops differentiation\n",
    "                # hid_ref = masked_tensor(hid_ref, mask.to(model.device), requires_grad=True).to_tensor(value=0)\n",
    "                # hid_cur = masked_tensor(hid_cur, mask.to(model.device), requires_grad=True).to_tensor(value=0)\n",
    "\n",
    "                # multiply by mask to zero-out non-target elements\n",
    "                hid_ref = hid_ref*batch[2].unsqueeze(-1).to(model.device)\n",
    "                hid_cur = hid_cur*batch[3].unsqueeze(-1).to(model.device)\n",
    "            \n",
    "            hid_ref = torch.mean(hid_ref, dim=1)\n",
    "            hid_cur = torch.mean(hid_cur, dim=1)\n",
    "\n",
    "            # compute batch anisotropy\n",
    "            local_anisotropy = executor.submit(SVDmatrix.compute_anisotropy, matrix=(hid_cur-hid_ref).detach().cpu())\n",
    "            \n",
    "            ###########################################\n",
    "            ## CHANGE IN CASE DIFFERENT LOSS IS USED ##\n",
    "            ###########################################\n",
    "            total_loss = criteria(hid_ref, hid_cur, target)\n",
    "            total_loss.backward()\n",
    "            total_grad_norm = gradient_norm()\n",
    "            \n",
    "            # as indexing in weight_loss starts from -1, take a step before applying            \n",
    "            weight_loss.step()\n",
    "\n",
    "            change_gradients(model=model, \n",
    "                             layers=grads, \n",
    "                             division_layer=division_layer,\n",
    "                             weight_mlm=weight_mlm, \n",
    "                             weight_loss=weight_loss,\n",
    "                            logger=logger)\n",
    "                \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()                    \n",
    "\n",
    "            total_losses.append(total_loss.detach().cpu())\n",
    "\n",
    "            # some averaged estimate to report\n",
    "            total_loss = (sum(total_losses[-30:]) / len(total_losses[-30:])).item()\n",
    "\n",
    "            tda_feats_base = read_tda_feats(path.joinpath(\"base.tda_feats\"))\n",
    "            tda_feats_poly = read_tda_feats(path.joinpath(\"polypers.tda_feats\"))\n",
    "            \n",
    "\n",
    "            # TODO: possibly send not every batch -- however, it acts asynchrously, so doesn't seem to make much difference\n",
    "            wandb.log({\"Epoch\": epoch,\n",
    "                       \"MLM loss\": mlm_loss,\n",
    "                       \"Cosine loss\": total_loss,\n",
    "                       \"MLM grad norm\": mlm_grad_norm,\n",
    "                       \"Additional grad norm\": total_grad_norm,\n",
    "                       \"Cos Weight\": weight_loss.weight, # may be not that necessary to report unless we chaned it somehow\n",
    "                       \"Learning rate\": lr_scheduler.get_last_lr()[0],\n",
    "                       \"Local anisotropy\": local_anisotropy.result(),\n",
    "                       \"Global anisotropy base\": float(get_last_line(path.joinpath(\"base.anisotropy\"))),\n",
    "                       \"Global anisotropy polypers\": float(get_last_line(path.joinpath(\"polypers.anisotropy\"))),\n",
    "                       \"Global persistent dimension base\": float(get_last_line(path.joinpath(\"base.ph_dim\"))),\n",
    "                       \"Global persistent dimension polypers\": float(get_last_line(path.joinpath(\"polypers.ph_dim\"))),\n",
    "                       \"Global persistent entropy base\": tda_feats_base.entropy,\n",
    "                       \"Global persistent barcode mean base\": tda_feats_base.mean,\n",
    "                       \"Global persistent barcode std base\": tda_feats_base.std,\n",
    "                       \"Global persistent entropy polypers\": tda_feats_poly.entropy,\n",
    "                       \"Global persistent barcode mean polypers\": tda_feats_poly.mean,\n",
    "                       \"Global persistent barcode std polypers\": tda_feats_poly.std,\n",
    "                       \"TDA test persistent dimension base\": test_base_pers_dim.result(),\n",
    "                       \"TDA test persistent dimension polypers\": test_polypers_pers_dim.result(),\n",
    "                       \"TDA test persistent entropy base\": test_base_tda_feats.result().entropy,\n",
    "                       \"TDA test persistent barcode mean base\": test_base_tda_feats.result().mean,\n",
    "                       \"TDA test persistent barcode std\": test_base_tda_feats.result().std,\n",
    "                       \"TDA test persistent entropy polypers\": test_polypers_tda_feats.result().entropy,\n",
    "                       \"TDA test persistent barcode mean polypers\": test_polypers_tda_feats.result().mean,\n",
    "                       \"TDA test persistent entropy polypers\": test_polypers_tda_feats.result().std,\n",
    "                      })\n",
    "            tq_batch.set_postfix({\n",
    "                    'MLM loss': mlm_loss,\n",
    "                    'Additional loss': total_loss\n",
    "                })\n",
    "\n",
    "            tq_batch.update(1)\n",
    "\n",
    "        if epoch % save_every_epoch == 0:\n",
    "            # Note -- we don't save the model class, only the weights\n",
    "            save_model(epoch, model, optimizer, pred, total_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "SPNBA0emMpWr"
   },
   "outputs": [],
   "source": [
    "dt = PairsDataset(tokenizer, path=DATA_PATH)\n",
    "dl = DataLoader(dt, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                collate_fn=collate_func, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "SPNBA0emMpWr"
   },
   "outputs": [],
   "source": [
    "dt_tda = PairsDataset(tokenizer, path=TEST_PATH)\n",
    "dl_tda = DataLoader(dt_tda, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                    collate_fn=tda_collate_func,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Compute SVD on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVDmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_svd = SVDmatrix.GetSVD(model=model, dataloader=dl, division_layer=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_svd.get_matrix(use_masks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(get_svd.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minio.put_object(get_svd.matrix, save_name=\"matrices/3rd_layer_diff_matrix_masked.pkl\", pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_svd.compute_svd(compute_uv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minio.put_object(get_svd.svd, save_name=\"matrices/3rd_layer_diff_svd_masked.pkl\", pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different solution (when library internal tobytes interface is implemented --\n",
    "# this variant is prefered)\n",
    "\n",
    "# minio.put_object(get_svd.svd[0].tobytes(), save_name=\"svd_test\", pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "CAq3W2Qd3c3y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scale by number of steps in a cycle\n",
    "weight_mlm /= n_cosine\n",
    "\n",
    "# reassign weight loss to custom class implementation\n",
    "weight_loss = LossWeightSum2One(init_coef=1)\n",
    "\n",
    "# set weight to constant \n",
    "# weight_loss = LossWeightDecay(init_state=weight_loss/n_cosine, decay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "vncR8UNMvk4C"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = name.startswith(f\"bert.encoder.layer.{division_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "mSRwfobbRsQY"
   },
   "outputs": [],
   "source": [
    "# vec = torch.normal(0.5,\n",
    "#                    0.1,\n",
    "#                    size=(768, ),\n",
    "#                    requires_grad=False).repeat(BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "O48t9N1K8XCF"
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=learning_rate)\n",
    "# criterion = CosLoss(alpha=0.5, vec=vec)\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "# criterion = ProjectionLoss(subspace_matrix=proj, weight_cosine=weight_cosine, weight_projection=weight_projection)\n",
    "\n",
    "num_training_steps = int(num_epochs * len(dl) / n_cosine * n_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43732"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of lr sheduler updates\n",
    "num_training_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate & averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_SWA:\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "                    name=\"cosine\", optimizer=optimizer, num_warmup_steps=num_warmup_steps,\n",
    "                    num_training_steps=num_training_steps\n",
    "                    )\n",
    "\n",
    "    # make steps at the end of first 5 epochs -- may be object to change made explicitely\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "    #                                                     milestones=[int(len(dl)*i/n_cosine*n_mlm) for i in range(1, num_epochs)],\n",
    "    #                                                     gamma=0.8,)\n",
    "    \n",
    "    # # Note: Averaged model is applicable to custom modules -- not only full models, \n",
    "    # so it can be used for module training as well\n",
    "    if USE_EMA:\n",
    "        avg_model = torch.optim.swa_utils.AveragedModel(model,  \n",
    "                                                multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(decay=EMA_DECAY)).to(device)\n",
    "\n",
    "elif USE_SWA:\n",
    "    avg_model = torch.optim.swa_utils.AveragedModel(model,\n",
    "                                           multi_avg_fn=torch.optim.swa_utils.get_swa_multi_avg_fn()).to(device)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                        milestones=[int(len(dl)*i/n_cosine*n_mlm)for i in range(1, num_epochs)],\n",
    "                                                        gamma=0.5,)\n",
    "    # ex. after 5 epochs\n",
    "    logger.info(f\"Using swa after {SWA_START} epochs\")\n",
    "    # grows swa learning rate from current optimizer value till the SWA_lr,\n",
    "    # anneal epochs determines the number of steps necessary\n",
    "    # may be it's plausible to change from our current implementation to the epoch-wise stepping\n",
    "    swa_scheduler = torch.optim.swa_utils.SWALR(optimizer=optimizer, \n",
    "                                    swa_lr=SWA_LR,\n",
    "                                    anneal_strategy=\"cos\",\n",
    "                                    anneal_epochs=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NAIVE_COSINE_WITH_PRETRAINED_BERT_EMA_UPDATED_MASKS\n",
      "\n",
      "Description: Based on BERT pretrained for 10 epochs -- cosine loss with no additions, trained using EMA;\n",
      "\n",
      "Model: based on rubert, additionally pretrained for 10 epochs;\n",
      "Checkpoint: ckpt/pretrained_bert/model_epoch_10.pt;\n",
      "Context: 64;\n",
      "Batch size: 16;\n",
      "\n",
      "Loss: Cosine embedding loss; \n",
      "MLM type: token masking;\n",
      "\n",
      "Loss only over diff in tokenization: True;\n",
      "Compute test tda on masked tokens only: True;\n",
      "N_MLM: 1;\n",
      "N_Cosine: 10;\n",
      "Division_layer: 3;\n",
      "weight_mlm: 1.2;\n",
      "weight_loss: 1, polynomial decay, sum to one;\n",
      "\n",
      "\n",
      "LR_SCHEDULER: Cosine annealing with warmup;\n",
      "Initial learning rate: 0.005;\n",
      "Warmup_steps: 3000;\n",
      "Steps: 13, \n",
      "Decay: cosine,\n",
      "Epochs: 13;\n",
      "\n",
      "Additional parameters and notes:\n",
      "EMA: True;\n",
      "EMA_DECAY: 0.995;\n",
      "\n",
      "SWA: False;\n",
      "SWA_START: 4;\n",
      "SWA_LR: 0.05;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396,
     "referenced_widgets": [
      "001cfa81c7354e93be3df3a5cf1b1058",
      "1957ad156733493da7a1d98af4e41f75",
      "b09cac7f786d41508fc087bfdc5967b7",
      "3adea4ff86b3452ea61fa306820297e6",
      "644be7a3004544af89d53d40dfad2f2b",
      "ab1566b360054b28942a3037cf3b746a",
      "36d21aa4eb7245f7918062cea3ea7a1b",
      "99f7527310794081b4b6adf4b6f3d4eb",
      "02c75806a08d4fd48c35b5a1dcdea1fb",
      "4064efa851f842cdb64b85b7498e2072",
      "9cd5f89775324214b3bcf755a575bf56",
      "7145096b67ab4dc69573e9047570c587",
      "e465332bfe814a2485071916e79d6ec0",
      "f468a1aab83f440a9a49912cb2cf901e",
      "cf17fde9c27f40a4b4e8ee7a83918f96",
      "e60d7f63aedf4b52982c846515cbc703",
      "991f151a7fe041e39275a97cdc1d2838",
      "edf64befa1a3437f82ca7886bb27b9af",
      "4c82641162ed4e569484ba69cbc255c6",
      "e1441e83a1f84e448f7a4b19dc1e6527",
      "99f562bfbd5c4f25a847a01add5d776a",
      "f76d3bb75a774ce9a538561618ca09cf"
     ]
    },
    "id": "F762mn4J1t5F",
    "outputId": "51fddd23-30ec-446f-fcde-ae5f7db8f13e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModularLM/ckpt/naive_cosine_with_pretrained_bert_EMA_updated_masks/DESCRIPTION.txt: |####################| 0.00 MB/0.00 MB 100% [elapsed: 00:00 left: 00:00,  0.32 MB/sec]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1f3387e4ae4eb9bec00665fa7ad44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ea3eb53e614b15992106f4beeae084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModularLM/data/TDA_FEATURES/tda_save_dict_naive_cosine_with_pretrained_bert_EMA_updated_masks.pkl: |####################| 9.95 MB/9.95 MB 100% [elapsed: 00:00 left: 00:00, 397.94 MB/sec] ]"
     ]
    }
   ],
   "source": [
    "# put description to minio before proceeding\n",
    "minio.put_object(DESCRIPTION, \n",
    "                save_name=f\"ckpt/{TRY_NAME}/DESCRIPTION.txt\", pickle=True)\n",
    "\n",
    "# train model\n",
    "tda_save_dict = defaultdict(dict)\n",
    "\n",
    "try:\n",
    "    train(model=model, criteria=criterion, optimizer=optimizer,\n",
    "          lr_scheduler=lr_scheduler, data=dl, hom_data=dl_tda,\n",
    "          n_epochs=num_epochs,\n",
    "          n_cosine=n_cosine, division_layer=division_layer+1,\n",
    "          weight_mlm=weight_mlm, weight_loss=weight_loss,\n",
    "          save_every_epoch=2, test_every=3000)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    minio.put_object(DESCRIPTION, \n",
    "                save_name=f\"ckpt/trained_models/{TRY_NAME}/DESCRIPTION.txt\", pickle=True)\n",
    "    # in case we use averaging optimization -- we need to recalculate batch | layer normalization before inference\n",
    "    if (USE_EMA | USE_SWA):\n",
    "        torch.optim.swa_utils.update_bn(dl, avg_model)\n",
    "    \n",
    "    # save to trained models\n",
    "    logger.info(\"Saving model...\")\n",
    "    buffer = io.BytesIO()\n",
    "    \n",
    "    if (USE_EMA | USE_SWA):\n",
    "        torch.save({\n",
    "            'model_state_dict': avg_model.state_dict(),\n",
    "                    }, f=buffer)\n",
    "    else:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "                    }, f=buffer)\n",
    "    \n",
    "    minio.put_object(buffer.getvalue(), \n",
    "                     save_name=f\"ckpt/trained_models/{TRY_NAME}/model.pt\")\n",
    "    \n",
    "    minio.fput_object(file_path=path.joinpath(\"base.anisotropy\"),\n",
    "                      object_name=f\"ModularLM/ckpt/trained_models/{TRY_NAME}/base.anisotropy.log\")\n",
    "    \n",
    "    minio.fput_object(file_path=path.joinpath(\"polypers.anisotropy\"),\n",
    "                      object_name=f\"ModularLM/ckpt/trained_models/{TRY_NAME}/polypers.anisotropy.log\", )\n",
    "    \n",
    "    minio.fput_object(file_path=path.joinpath(\"base.ph_dim\"),\n",
    "                      object_name=f\"ModularLM/ckpt/trained_models/{TRY_NAME}/base.ph_dim.log\")\n",
    "    minio.fput_object(file_path=path.joinpath(\"polypers.ph_dim\"),\n",
    "                      object_name=f\"ModularLM/ckpt/trained_models/{TRY_NAME}/polypers.ph_dim.log\")\n",
    "    minio.fput_object(file_path=path.joinpath(\"base.tda_feats\"),\n",
    "                      object_name=f\"ModularLM/ckpt/trained_models/{TRY_NAME}/base.tda_feats.log\")\n",
    "    minio.fput_object(file_path=path.joinpath(\"polypers.tda_feats\"),\n",
    "                      object_name=f\"ModularLM/ckpt/trained_models/{TRY_NAME}/polypers.tda_feats.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001cfa81c7354e93be3df3a5cf1b1058": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1957ad156733493da7a1d98af4e41f75",
       "IPY_MODEL_b09cac7f786d41508fc087bfdc5967b7",
       "IPY_MODEL_3adea4ff86b3452ea61fa306820297e6"
      ],
      "layout": "IPY_MODEL_644be7a3004544af89d53d40dfad2f2b"
     }
    },
    "02c75806a08d4fd48c35b5a1dcdea1fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1957ad156733493da7a1d98af4e41f75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab1566b360054b28942a3037cf3b746a",
      "placeholder": "​",
      "style": "IPY_MODEL_36d21aa4eb7245f7918062cea3ea7a1b",
      "value": "Epochs:   0%"
     }
    },
    "36d21aa4eb7245f7918062cea3ea7a1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3adea4ff86b3452ea61fa306820297e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4064efa851f842cdb64b85b7498e2072",
      "placeholder": "​",
      "style": "IPY_MODEL_9cd5f89775324214b3bcf755a575bf56",
      "value": " 0/1 [00:56&lt;?, ?it/s]"
     }
    },
    "4064efa851f842cdb64b85b7498e2072": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c82641162ed4e569484ba69cbc255c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "644be7a3004544af89d53d40dfad2f2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7145096b67ab4dc69573e9047570c587": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e465332bfe814a2485071916e79d6ec0",
       "IPY_MODEL_f468a1aab83f440a9a49912cb2cf901e",
       "IPY_MODEL_cf17fde9c27f40a4b4e8ee7a83918f96"
      ],
      "layout": "IPY_MODEL_e60d7f63aedf4b52982c846515cbc703"
     }
    },
    "991f151a7fe041e39275a97cdc1d2838": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99f562bfbd5c4f25a847a01add5d776a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99f7527310794081b4b6adf4b6f3d4eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cd5f89775324214b3bcf755a575bf56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab1566b360054b28942a3037cf3b746a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b09cac7f786d41508fc087bfdc5967b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99f7527310794081b4b6adf4b6f3d4eb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_02c75806a08d4fd48c35b5a1dcdea1fb",
      "value": 0
     }
    },
    "cf17fde9c27f40a4b4e8ee7a83918f96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99f562bfbd5c4f25a847a01add5d776a",
      "placeholder": "​",
      "style": "IPY_MODEL_f76d3bb75a774ce9a538561618ca09cf",
      "value": " 166/37379 [00:55&lt;2:56:01,  3.52it/s, Cosine loss=1.46]"
     }
    },
    "e1441e83a1f84e448f7a4b19dc1e6527": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e465332bfe814a2485071916e79d6ec0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_991f151a7fe041e39275a97cdc1d2838",
      "placeholder": "​",
      "style": "IPY_MODEL_edf64befa1a3437f82ca7886bb27b9af",
      "value": "  0%"
     }
    },
    "e60d7f63aedf4b52982c846515cbc703": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edf64befa1a3437f82ca7886bb27b9af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f468a1aab83f440a9a49912cb2cf901e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c82641162ed4e569484ba69cbc255c6",
      "max": 37379,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1441e83a1f84e448f7a4b19dc1e6527",
      "value": 166
     }
    },
    "f76d3bb75a774ce9a538561618ca09cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
