{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f2e6dc-f7ee-4e8d-9089-32bd090b3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74fb81e0-0ae4-47b4-84f7-efc4a367a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675b0346-b198-449c-9540-7e62dd23aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8cff4de-135b-4690-83e4-90355cdb1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02af32d6-c717-4623-8320-003c68a67a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "070a56ee-c418-4fb8-b4a7-9af57e329ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "class ModifiedModel(nn.Module):\n",
    "    def __init__(self, pretrained_model: nn.Module, custom_module: nn.Module, insert_after_layer: int, \n",
    "                 debug: bool = False):\n",
    "        super(ModifiedModel, self).__init__()\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.pretrained_model = pretrained_model\n",
    "        \n",
    "        self.features = nn.Sequential(*list(self.pretrained_model.children())[:insert_after_layer])\n",
    "        \n",
    "        self.remaining_layers = nn.Sequential(*list(self.pretrained_model.children())[insert_after_layer:-1])\n",
    "\n",
    "        self.linear = list(self.pretrained_model.children())[-1]\n",
    "        \n",
    "        self.custom_module = custom_module\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.features(x)\n",
    "        if self.debug:\n",
    "            print(x.size())\n",
    "        \n",
    "        \n",
    "        x = self.custom_module(x)\n",
    "        if self.debug:\n",
    "            print(x.size())\n",
    "        \n",
    "        \n",
    "        x = self.remaining_layers(x)\n",
    "        if self.debug:\n",
    "            print(x.size())\n",
    "\n",
    "        x = torch.squeeze(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "pretrained_resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "custom_module = nn.Conv2d(64, 64, kernel_size=3, padding=1) \n",
    "\n",
    "insert_after_layer = 3  \n",
    "\n",
    "modified_model = ModifiedModel(pretrained_model=pretrained_resnet, \n",
    "                               custom_module=custom_module, insert_after_layer=insert_after_layer,\n",
    "                                debug = True)\n",
    "\n",
    "\n",
    "input_data = torch.randn(1, 3, 512, 512) \n",
    "output = pretrained_resnet(input_data)\n",
    "print(output.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9706949c-c6c7-4678-b78d-979770e6fc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 1000]                 --\n",
       "├─Conv2d: 1-1                            [1, 64, 256, 256]         9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 256, 256]         128\n",
       "├─ReLU: 1-3                              [1, 64, 256, 256]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 128, 128]         --\n",
       "├─Sequential: 1-5                        [1, 64, 128, 128]         --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 128, 128]         --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 128, 128]         --\n",
       "├─Sequential: 1-6                        [1, 128, 64, 64]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 64, 64]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 64, 64]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 64, 64]          256\n",
       "│    │    └─Sequential: 3-18             [1, 128, 64, 64]          8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 64, 64]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 64, 64]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 64, 64]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 64, 64]          --\n",
       "├─Sequential: 1-7                        [1, 256, 32, 32]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 32, 32]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 32, 32]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-31             [1, 256, 32, 32]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 32, 32]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 32, 32]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 32, 32]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 32, 32]          --\n",
       "├─Sequential: 1-8                        [1, 512, 16, 16]          --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 16, 16]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 16, 16]          1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 16, 16]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-44             [1, 512, 16, 16]          132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 16, 16]          --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 16, 16]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 16, 16]          1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 16, 16]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 16, 16]          1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 16, 16]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 1000]                 513,000\n",
       "==========================================================================================\n",
       "Total params: 11,689,512\n",
       "Trainable params: 11,689,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 9.48\n",
       "==========================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 207.63\n",
       "Params size (MB): 46.76\n",
       "Estimated Total Size (MB): 257.53\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(pretrained_resnet, (1, 3, 512, 512), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "114322f7-b866-438e-afac-cda98159ff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 512, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ModifiedModel                                 [1000]                    --\n",
       "├─Sequential: 1-1                             [1, 64, 256, 256]         --\n",
       "│    └─Conv2d: 2-1                            [1, 64, 256, 256]         9,408\n",
       "│    └─BatchNorm2d: 2-2                       [1, 64, 256, 256]         128\n",
       "│    └─ReLU: 2-3                              [1, 64, 256, 256]         --\n",
       "├─Conv2d: 1-2                                 [1, 64, 256, 256]         36,928\n",
       "├─Sequential: 1-3                             [1, 512, 1, 1]            --\n",
       "│    └─MaxPool2d: 2-4                         [1, 64, 128, 128]         --\n",
       "│    └─Sequential: 2-5                        [1, 64, 128, 128]         --\n",
       "│    │    └─BasicBlock: 3-1                   [1, 64, 128, 128]         73,984\n",
       "│    │    └─BasicBlock: 3-2                   [1, 64, 128, 128]         73,984\n",
       "│    └─Sequential: 2-6                        [1, 128, 64, 64]          --\n",
       "│    │    └─BasicBlock: 3-3                   [1, 128, 64, 64]          230,144\n",
       "│    │    └─BasicBlock: 3-4                   [1, 128, 64, 64]          295,424\n",
       "│    └─Sequential: 2-7                        [1, 256, 32, 32]          --\n",
       "│    │    └─BasicBlock: 3-5                   [1, 256, 32, 32]          919,040\n",
       "│    │    └─BasicBlock: 3-6                   [1, 256, 32, 32]          1,180,672\n",
       "│    └─Sequential: 2-8                        [1, 512, 16, 16]          --\n",
       "│    │    └─BasicBlock: 3-7                   [1, 512, 16, 16]          3,673,088\n",
       "│    │    └─BasicBlock: 3-8                   [1, 512, 16, 16]          4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-4                                 [1000]                    513,000\n",
       "===============================================================================================\n",
       "Total params: 11,726,440\n",
       "Trainable params: 11,726,440\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 12.41\n",
       "===============================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 241.18\n",
       "Params size (MB): 46.91\n",
       "Estimated Total Size (MB): 291.23\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(modified_model, (1, 3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6530f69-2a86-4e31-8663-aea7edc25367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bdc9bb8b-2568-40d6-a08d-b911168807c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = bert_model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a031f9ae-f60b-47bf-ab38-0276bf132a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_module(model, indices, modules):\n",
    "    indices = indices if isinstance(indices, list) else [indices]\n",
    "    modules = modules if isinstance(modules, list) else [modules]\n",
    "    assert len(indices) == len(modules)\n",
    "\n",
    "    layers_name = [name for name, _ in model.named_modules()][1:]\n",
    "    for index, module in zip(indices, modules):\n",
    "        layer_name = re.sub(r'(.)(\\d)', r'[\\2]', layers_name[index])\n",
    "        exec(\"model.{name} = nn.Sequential(model.{name}, module)\".format(name = layer_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961bb6fe-34e5-4824-b4e9-1efd943741ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa4fcb84-d189-4074-90e7-b9a3c98f9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_name = [name for name, _ in model.named_modules()][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e09513e-0edf-4467-a61e-6231b363906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embeddings',\n",
       " 'embeddings.word_embeddings',\n",
       " 'embeddings.position_embeddings',\n",
       " 'embeddings.LayerNorm',\n",
       " 'embeddings.dropout',\n",
       " 'transformer',\n",
       " 'transformer.layer',\n",
       " 'transformer.layer.0',\n",
       " 'transformer.layer.0.attention',\n",
       " 'transformer.layer.0.attention.dropout',\n",
       " 'transformer.layer.0.attention.q_lin',\n",
       " 'transformer.layer.0.attention.k_lin',\n",
       " 'transformer.layer.0.attention.v_lin',\n",
       " 'transformer.layer.0.attention.out_lin',\n",
       " 'transformer.layer.0.sa_layer_norm',\n",
       " 'transformer.layer.0.ffn',\n",
       " 'transformer.layer.0.ffn.dropout',\n",
       " 'transformer.layer.0.ffn.lin1',\n",
       " 'transformer.layer.0.ffn.lin2',\n",
       " 'transformer.layer.0.ffn.activation',\n",
       " 'transformer.layer.0.output_layer_norm',\n",
       " 'transformer.layer.1',\n",
       " 'transformer.layer.1.attention',\n",
       " 'transformer.layer.1.attention.dropout',\n",
       " 'transformer.layer.1.attention.q_lin',\n",
       " 'transformer.layer.1.attention.k_lin',\n",
       " 'transformer.layer.1.attention.v_lin',\n",
       " 'transformer.layer.1.attention.out_lin',\n",
       " 'transformer.layer.1.sa_layer_norm',\n",
       " 'transformer.layer.1.ffn',\n",
       " 'transformer.layer.1.ffn.dropout',\n",
       " 'transformer.layer.1.ffn.lin1',\n",
       " 'transformer.layer.1.ffn.lin2',\n",
       " 'transformer.layer.1.ffn.activation',\n",
       " 'transformer.layer.1.output_layer_norm',\n",
       " 'transformer.layer.2',\n",
       " 'transformer.layer.2.attention',\n",
       " 'transformer.layer.2.attention.dropout',\n",
       " 'transformer.layer.2.attention.q_lin',\n",
       " 'transformer.layer.2.attention.k_lin',\n",
       " 'transformer.layer.2.attention.v_lin',\n",
       " 'transformer.layer.2.attention.out_lin',\n",
       " 'transformer.layer.2.sa_layer_norm',\n",
       " 'transformer.layer.2.ffn',\n",
       " 'transformer.layer.2.ffn.dropout',\n",
       " 'transformer.layer.2.ffn.lin1',\n",
       " 'transformer.layer.2.ffn.lin2',\n",
       " 'transformer.layer.2.ffn.activation',\n",
       " 'transformer.layer.2.output_layer_norm',\n",
       " 'transformer.layer.3',\n",
       " 'transformer.layer.3.attention',\n",
       " 'transformer.layer.3.attention.dropout',\n",
       " 'transformer.layer.3.attention.q_lin',\n",
       " 'transformer.layer.3.attention.k_lin',\n",
       " 'transformer.layer.3.attention.v_lin',\n",
       " 'transformer.layer.3.attention.out_lin',\n",
       " 'transformer.layer.3.sa_layer_norm',\n",
       " 'transformer.layer.3.ffn',\n",
       " 'transformer.layer.3.ffn.dropout',\n",
       " 'transformer.layer.3.ffn.lin1',\n",
       " 'transformer.layer.3.ffn.lin2',\n",
       " 'transformer.layer.3.ffn.activation',\n",
       " 'transformer.layer.3.output_layer_norm',\n",
       " 'transformer.layer.4',\n",
       " 'transformer.layer.4.attention',\n",
       " 'transformer.layer.4.attention.dropout',\n",
       " 'transformer.layer.4.attention.q_lin',\n",
       " 'transformer.layer.4.attention.k_lin',\n",
       " 'transformer.layer.4.attention.v_lin',\n",
       " 'transformer.layer.4.attention.out_lin',\n",
       " 'transformer.layer.4.sa_layer_norm',\n",
       " 'transformer.layer.4.ffn',\n",
       " 'transformer.layer.4.ffn.dropout',\n",
       " 'transformer.layer.4.ffn.lin1',\n",
       " 'transformer.layer.4.ffn.lin2',\n",
       " 'transformer.layer.4.ffn.activation',\n",
       " 'transformer.layer.4.output_layer_norm',\n",
       " 'transformer.layer.5',\n",
       " 'transformer.layer.5.attention',\n",
       " 'transformer.layer.5.attention.dropout',\n",
       " 'transformer.layer.5.attention.q_lin',\n",
       " 'transformer.layer.5.attention.k_lin',\n",
       " 'transformer.layer.5.attention.v_lin',\n",
       " 'transformer.layer.5.attention.out_lin',\n",
       " 'transformer.layer.5.sa_layer_norm',\n",
       " 'transformer.layer.5.ffn',\n",
       " 'transformer.layer.5.ffn.dropout',\n",
       " 'transformer.layer.5.ffn.lin1',\n",
       " 'transformer.layer.5.ffn.lin2',\n",
       " 'transformer.layer.5.ffn.activation',\n",
       " 'transformer.layer.5.output_layer_norm']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd5d0c13-460f-4b6d-af9d-5e9611bd0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_part(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin = nn.Linear(in_features=in_dim, out_features=out_dim)\n",
    "        self.laynorm = nn.LayerNorm(out_dim)\n",
    "        self.act = nn.ReLU6()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.lin(input)\n",
    "        x = self.laynorm(self.act(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "652d2f0a-c740-4f3e-bd6c-1cf4544d83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_module(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_size: int, internal_size: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = sparse_part(layer_size, internal_size)\n",
    "\n",
    "        self.bootleneck = sparse_part(internal_size, internal_size//8)\n",
    "\n",
    "        self.output = nn.Linear(internal_size//8, layer_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.layer(input)\n",
    "        x = self.bottleneck(self.lin2(input))\n",
    "        x = nn.functional.leaky_relu(self.output(x))\n",
    "\n",
    "        return x, nn.functional.l1_loss(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a68dc2af-f848-411a-ac51-c34abaa21d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedModel(nn.Module):\n",
    "    def __init__(self, pretrained_model: nn.Module, custom_module: nn.Module, insert_after_layer: int, \n",
    "                 debug: bool = False):\n",
    "        super(ModifiedModel, self).__init__()\n",
    "        self.debug = debug\n",
    "        \n",
    "\n",
    "        self.pretrained_model = pretrained_model\n",
    "\n",
    "        self.embedding = list(self.pretrained_model.children())[0]\n",
    "        \n",
    "\n",
    "        self.features = nn.Sequential(*list(self.pretrained_model.children())[1:insert_after_layer])\n",
    "        \n",
    "\n",
    "        self.remaining_layers = nn.Sequential(*list(self.pretrained_model.children())[insert_after_layer:])\n",
    "        \n",
    "        self.custom_module = custom_module\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        print(self.embedding)\n",
    "        \n",
    "        enc = self.embedding(x[\"input_ids\"])\n",
    "        if self.debug:\n",
    "            print(x, enc.size())\n",
    "            \n",
    "        x = self.features({\"hidden_states\": enc, \"attention_mask\": x[\"attention_mask\"]})\n",
    "        if self.debug:\n",
    "            print(x.size())\n",
    "        \n",
    "\n",
    "        x = self.custom_module(x)\n",
    "        if self.debug:\n",
    "            print(x.size())\n",
    "\n",
    "        x = self.remaining_layers(x)\n",
    "        if self.debug:\n",
    "            print(x.size())\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a297e76f-ae95-49b9-b093-75990f4df3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_module = sparse_module(768, 96) \n",
    "\n",
    "insert_after_layer = 33\n",
    "\n",
    "modified_model = ModifiedModel(pretrained_model=bert_model, \n",
    "                               custom_module=custom_module, insert_after_layer=insert_after_layer,\n",
    "                                debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "676e7666-2f8a-4b75-8610-85ba20531742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('transformer.layer.0.ffn',\n",
       " FFN(\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (activation): GELUActivation()\n",
       " ))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(modified_model.pretrained_model.named_modules())[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ce070a78-102a-49b8-ba30-9c3b68373311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "{'input_ids': tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} torch.Size([1, 12, 768])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[132], line 28\u001b[0m, in \u001b[0;36mModifiedModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x, enc\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_states\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:590\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    580\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    581\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m         output_attentions,\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    588\u001b[0m         hidden_state,\n\u001b[1;32m    589\u001b[0m         attn_mask,\n\u001b[0;32m--> 590\u001b[0m         \u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    591\u001b[0m         output_attentions,\n\u001b[1;32m    592\u001b[0m     )\n\u001b[1;32m    594\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "output = modified_model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4d80f-9b82-4d3d-978f-506473c694a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a6ec6170-03a1-4e22-b98f-4f4eff406c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4591e74-179e-47dc-ab5b-34a3be9fde59",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_module(model.pretrained, 33, sparse_module(768, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30261ff8-c233-4362-92ab-db2f2d265b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a95de7c-3c7f-409d-b513-0cc0dc42efca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2d9bcd12-831a-43de-bc33-239865fab11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoded_input.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "545394c0-e015-49d2-8d41-d38e2005359a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 12]), torch.Size([1, 12])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[val.shape for val in  encoded_input.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1700f2-ae67-4abc-b700-786fbdac7ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3fdbda7-89a1-4d27-96d1-b6072c751c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4056b-f41b-48ce-9f72-37535e08d942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
