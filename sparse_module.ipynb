{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f2e6dc-f7ee-4e8d-9089-32bd090b3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fb81e0-0ae4-47b4-84f7-efc4a367a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675b0346-b198-449c-9540-7e62dd23aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8cff4de-135b-4690-83e4-90355cdb1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc9bb8b-2568-40d6-a08d-b911168807c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = bert_model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e96407-f01d-4fa4-99ed-89c8e53c450a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a031f9ae-f60b-47bf-ab38-0276bf132a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_module(model, indices, modules):\n",
    "    indices = indices if isinstance(indices, list) else [indices]\n",
    "    modules = modules if isinstance(modules, list) else [modules]\n",
    "    assert len(indices) == len(modules)\n",
    "\n",
    "    layers_name = [name for name, _ in model.named_modules()][1:]\n",
    "    for index, module in zip(indices, modules):\n",
    "        layer_name = re.sub(r'(.)(\\d)', r'[\\2]', layers_name[index])\n",
    "        exec(\"model.{name} = nn.Sequential(model.{name}, module)\".format(name = layer_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961bb6fe-34e5-4824-b4e9-1efd943741ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5d0c13-460f-4b6d-af9d-5e9611bd0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_part(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin = nn.Linear(in_features=in_dim, out_features=out_dim)\n",
    "        self.laynorm = nn.LayerNorm(out_dim)\n",
    "        self.act = nn.ReLU6()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.lin(input)\n",
    "        x = self.laynorm(self.act(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "652d2f0a-c740-4f3e-bd6c-1cf4544d83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_module(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_size: int, internal_size: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = sparse_part(layer_size, internal_size)\n",
    "\n",
    "        self.bottleneck = sparse_part(internal_size, internal_size//8)\n",
    "\n",
    "        self.output = nn.Linear(internal_size//8, layer_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.layer(input)\n",
    "        x = self.bottleneck(x)\n",
    "        x = nn.functional.leaky_relu(self.output(x))\n",
    "\n",
    "        return x, torch.linalg.vector_norm(x, ord = 1, dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a68dc2af-f848-411a-ac51-c34abaa21d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedModel(nn.Module):\n",
    "    def __init__(self, pretrained_model: nn.Module, custom_module: nn.Module, insert_after_layer: int, \n",
    "                 debug: bool = False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            pretrained_model: torch.nn.Module any pretrained model from HF (it may not work for any actually. Moreover, it may work for \n",
    "            the only specified here for now, and may require more suffisticated additional tricks to make it work for another\n",
    "            custom_module: torch.nn.Module The reqired module to add\n",
    "            insert_after_layer: int The number of block to insert after\n",
    "            debug: bool Show outputs of some layers during forward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ModifiedModel, self).__init__()\n",
    "        self.debug = debug\n",
    "        \n",
    "\n",
    "        self.pretrained_model = pretrained_model\n",
    "\n",
    "        self.embedding = list(self.pretrained_model.children())[0]\n",
    "\n",
    "        self.arr = nn.ModuleList([])\n",
    "\n",
    "        pattern = re.compile(\"transformer.layer.\\d+$\")\n",
    "        \n",
    "        for i in bert_model.named_modules():\n",
    "            if re.match(pattern, i[0]):\n",
    "                self.arr.append(i[1])\n",
    "        \n",
    "        self.custom_module = custom_module\n",
    "        self.insert_place = insert_after_layer\n",
    "\n",
    "    def forward(self, x: dict[torch.tensor]) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            x: dict(\"input_ids\": torch.tensor, \"attention_mask\": torch.tensor)\n",
    "        \n",
    "        Returns:\n",
    "            The output of encoder layers, without any linear classification layer at the end for now.\n",
    "        \"\"\"\n",
    "        \n",
    "        enc = self.embedding(x[\"input_ids\"])\n",
    "        if self.debug:\n",
    "            print(enc)\n",
    "\n",
    "        for module in self.arr[:self.insert_place]:\n",
    "            enc = module(**{\"x\": enc, \"attn_mask\": x[\"attention_mask\"]})[0]\n",
    "        \n",
    "        enc, l1_norm = self.custom_module(enc)\n",
    "        if self.debug:\n",
    "            print(enc.size(), l1_norm, l1_norm.size())\n",
    "\n",
    "        for module in self.arr[self.insert_place:]:\n",
    "            enc = module(**{\"x\": enc, \"attn_mask\": x[\"attention_mask\"]})[0]\n",
    "\n",
    "        \n",
    "        return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a297e76f-ae95-49b9-b093-75990f4df3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_module = sparse_module(768, 96) \n",
    "\n",
    "insert_after_layer = 2\n",
    "\n",
    "modified_model = ModifiedModel(pretrained_model=bert_model, \n",
    "                               custom_module=custom_module, insert_after_layer=insert_after_layer,\n",
    "                                debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75a602-da67-419f-ae24-b588adb0d8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fa4fcb84-d189-4074-90e7-b9a3c98f9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_name = [name for name, _ in modified_model.named_modules()][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "676e7666-2f8a-4b75-8610-85ba20531742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TransformerBlock(\n",
       "   (attention): MultiHeadSelfAttention(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       "   (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "   (ffn): FFN(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (activation): GELUActivation()\n",
       "   )\n",
       "   (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       " ),\n",
       " TransformerBlock(\n",
       "   (attention): MultiHeadSelfAttention(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       "   (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "   (ffn): FFN(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (activation): GELUActivation()\n",
       "   )\n",
       "   (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       " ),\n",
       " TransformerBlock(\n",
       "   (attention): MultiHeadSelfAttention(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       "   (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "   (ffn): FFN(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (activation): GELUActivation()\n",
       "   )\n",
       "   (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       " ),\n",
       " TransformerBlock(\n",
       "   (attention): MultiHeadSelfAttention(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       "   (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "   (ffn): FFN(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (activation): GELUActivation()\n",
       "   )\n",
       "   (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       " ),\n",
       " TransformerBlock(\n",
       "   (attention): MultiHeadSelfAttention(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       "   (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "   (ffn): FFN(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (activation): GELUActivation()\n",
       "   )\n",
       "   (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       " ),\n",
       " TransformerBlock(\n",
       "   (attention): MultiHeadSelfAttention(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       "   (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "   (ffn): FFN(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (activation): GELUActivation()\n",
       "   )\n",
       "   (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       " )]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(modified_model.arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ce070a78-102a-49b8-ba30-9c3b68373311",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = modified_model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4028206f-3e0c-4db8-ab6a-50c7e12192d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4618, -0.2675,  0.2585,  ..., -0.2905, -0.2198,  0.1461],\n",
       "         [-0.4584, -0.2628,  0.2569,  ..., -0.2902, -0.2140,  0.1538],\n",
       "         [-0.4588, -0.2500,  0.2531,  ..., -0.2852, -0.2159,  0.1659],\n",
       "         ...,\n",
       "         [-0.4604, -0.2616,  0.2504,  ..., -0.2881, -0.2142,  0.1562],\n",
       "         [-0.4467, -0.2545,  0.2575,  ..., -0.2914, -0.2174,  0.1486],\n",
       "         [-0.4562, -0.2612,  0.2558,  ..., -0.2924, -0.2115,  0.1512]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b9391-25e4-40e8-8e2d-8bf1efbe39ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc25fa2-7a5a-45d3-82f9-d6720a736535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c390d3aa-4c6c-455f-82ff-74fd563d8800",
   "metadata": {},
   "source": [
    "### another possible approach for basic architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02af32d6-c717-4623-8320-003c68a67a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "070a56ee-c418-4fb8-b4a7-9af57e329ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "class ModifiedModel(nn.Module):\n",
    "    def __init__(self, pretrained_model: nn.Module, custom_module: nn.Module, insert_after_layer: int, \n",
    "                 debug: bool = False):\n",
    "        super(ModifiedModel, self).__init__()\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.pretrained_model = pretrained_model\n",
    "        \n",
    "        self.features = nn.Sequential(*list(self.pretrained_model.children())[:insert_after_layer])\n",
    "        \n",
    "        self.remaining_layers = nn.Sequential(*list(self.pretrained_model.children())[insert_after_layer:-1])\n",
    "\n",
    "        self.linear = list(self.pretrained_model.children())[-1]\n",
    "        \n",
    "        self.custom_module = custom_module\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.features(x)\n",
    "        if self.debug:\n",
    "            print(x.size())\n",
    "        \n",
    "        \n",
    "        x = self.custom_module(x)\n",
    "        if self.debug:\n",
    "            print(x.size())\n",
    "        \n",
    "        \n",
    "        x = self.remaining_layers(x)\n",
    "        if self.debug:\n",
    "            print(x.size())\n",
    "\n",
    "        x = torch.squeeze(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "pretrained_resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "custom_module = nn.Conv2d(64, 64, kernel_size=3, padding=1) \n",
    "\n",
    "insert_after_layer = 3  \n",
    "\n",
    "modified_model = ModifiedModel(pretrained_model=pretrained_resnet, \n",
    "                               custom_module=custom_module, insert_after_layer=insert_after_layer,\n",
    "                                debug = True)\n",
    "\n",
    "\n",
    "input_data = torch.randn(1, 3, 512, 512) \n",
    "output = pretrained_resnet(input_data)\n",
    "print(output.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9706949c-c6c7-4678-b78d-979770e6fc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 1000]                 --\n",
       "├─Conv2d: 1-1                            [1, 64, 256, 256]         9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 256, 256]         128\n",
       "├─ReLU: 1-3                              [1, 64, 256, 256]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 128, 128]         --\n",
       "├─Sequential: 1-5                        [1, 64, 128, 128]         --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 128, 128]         --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 128, 128]         --\n",
       "├─Sequential: 1-6                        [1, 128, 64, 64]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 64, 64]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 64, 64]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 64, 64]          256\n",
       "│    │    └─Sequential: 3-18             [1, 128, 64, 64]          8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 64, 64]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 64, 64]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 64, 64]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 64, 64]          --\n",
       "├─Sequential: 1-7                        [1, 256, 32, 32]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 32, 32]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 32, 32]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-31             [1, 256, 32, 32]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 32, 32]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 32, 32]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 32, 32]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 32, 32]          --\n",
       "├─Sequential: 1-8                        [1, 512, 16, 16]          --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 16, 16]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 16, 16]          1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 16, 16]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-44             [1, 512, 16, 16]          132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 16, 16]          --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 16, 16]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 16, 16]          1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 16, 16]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 16, 16]          1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 16, 16]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 1000]                 513,000\n",
       "==========================================================================================\n",
       "Total params: 11,689,512\n",
       "Trainable params: 11,689,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 9.48\n",
       "==========================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 207.63\n",
       "Params size (MB): 46.76\n",
       "Estimated Total Size (MB): 257.53\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(pretrained_resnet, (1, 3, 512, 512), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "114322f7-b866-438e-afac-cda98159ff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 512, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ModifiedModel                                 [1000]                    --\n",
       "├─Sequential: 1-1                             [1, 64, 256, 256]         --\n",
       "│    └─Conv2d: 2-1                            [1, 64, 256, 256]         9,408\n",
       "│    └─BatchNorm2d: 2-2                       [1, 64, 256, 256]         128\n",
       "│    └─ReLU: 2-3                              [1, 64, 256, 256]         --\n",
       "├─Conv2d: 1-2                                 [1, 64, 256, 256]         36,928\n",
       "├─Sequential: 1-3                             [1, 512, 1, 1]            --\n",
       "│    └─MaxPool2d: 2-4                         [1, 64, 128, 128]         --\n",
       "│    └─Sequential: 2-5                        [1, 64, 128, 128]         --\n",
       "│    │    └─BasicBlock: 3-1                   [1, 64, 128, 128]         73,984\n",
       "│    │    └─BasicBlock: 3-2                   [1, 64, 128, 128]         73,984\n",
       "│    └─Sequential: 2-6                        [1, 128, 64, 64]          --\n",
       "│    │    └─BasicBlock: 3-3                   [1, 128, 64, 64]          230,144\n",
       "│    │    └─BasicBlock: 3-4                   [1, 128, 64, 64]          295,424\n",
       "│    └─Sequential: 2-7                        [1, 256, 32, 32]          --\n",
       "│    │    └─BasicBlock: 3-5                   [1, 256, 32, 32]          919,040\n",
       "│    │    └─BasicBlock: 3-6                   [1, 256, 32, 32]          1,180,672\n",
       "│    └─Sequential: 2-8                        [1, 512, 16, 16]          --\n",
       "│    │    └─BasicBlock: 3-7                   [1, 512, 16, 16]          3,673,088\n",
       "│    │    └─BasicBlock: 3-8                   [1, 512, 16, 16]          4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-4                                 [1000]                    513,000\n",
       "===============================================================================================\n",
       "Total params: 11,726,440\n",
       "Trainable params: 11,726,440\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 12.41\n",
       "===============================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 241.18\n",
       "Params size (MB): 46.91\n",
       "Estimated Total Size (MB): 291.23\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(modified_model, (1, 3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4d80f-9b82-4d3d-978f-506473c694a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4056b-f41b-48ce-9f72-37535e08d942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
